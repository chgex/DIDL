{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "\n",
    "import d2l_pytorch as d2l \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftn_blk(num_convs,in_channels,out_channels,dilation):\n",
    "    blk=[]\n",
    "    for i in range(num_convs):\n",
    "        # 卷积块中的第一个卷积层\n",
    "        if i ==0:\n",
    "            if dilation == 0:\n",
    "                blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3))\n",
    "            else:\n",
    "                blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,dilation=dilation))\n",
    "        else:\n",
    "            if dilation == 0:\n",
    "                blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3))\n",
    "            else:\n",
    "                blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,dilation=dilation))\n",
    "        blk.append(nn.ReLU(inplace=True))\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frontend_vgg(in_channels,num_class):\n",
    "    conv_arch=(\n",
    "        (2,in_channels, 64),\n",
    "        (2, 64,128),\n",
    "        (3,128,256),\n",
    "        (3,256,512),\n",
    "        (3,512,512)\n",
    "    ) \n",
    "    #  (conv nums of blk,in_channels,out_channels) \n",
    "    \n",
    "    dilations = [0, 0, 0, 0, 2, 4]\n",
    "    # dilation factor\n",
    "    \n",
    "    net=nn.Sequential()\n",
    "\n",
    "    for l,(convs_num,in_channels,out_channels) in enumerate(conv_arch):\n",
    "        blk=ftn_blk(convs_num,in_channels,out_channels,dilations[l])\n",
    "        if dilations[l+1]==0:\n",
    "            blk.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "        net.add_module('ftn_blk_'+str(l+1),nn.Sequential(*blk))\n",
    "        \n",
    "    # layer_6\n",
    "    net.add_module('ftn_fc6',nn.Sequential(\n",
    "            nn.Conv2d(out_channels,4096,kernel_size=7,dilation=dilations[5]),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5))\n",
    "    )\n",
    "    # layer_7\n",
    "    net.add_module('ftn_fc7',nn.Sequential(\n",
    "        nn.Conv2d(4096,4096,kernel_size=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.5))\n",
    "    )\n",
    "    # layer_8\n",
    "    net.add_module('ftn_fc8',nn.Conv2d(4096,num_class,kernel_size=1))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (ftn_blk_1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (ftn_blk_2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (ftn_blk_3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU(inplace=True)\n    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (ftn_blk_4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n    (5): ReLU(inplace=True)\n  )\n  (ftn_blk_5): Sequential(\n    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n    (3): ReLU(inplace=True)\n    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))\n    (5): ReLU(inplace=True)\n  )\n  (ftn_fc6): Sequential(\n    (0): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1), dilation=(4, 4))\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n  )\n  (ftn_fc7): Sequential(\n    (0): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n  )\n  (ftn_fc8): Conv2d(4096, 10, kernel_size=(1, 1), stride=(1, 1))\n)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "fn=frontend_vgg(3,10)\n",
    "print(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}