{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.10 批量归一化(batch normalization)\n",
    "\n",
    "关于BN：\n",
    "\n",
    "+ BN能让较深的神经网络的训练变得更加容易。\n",
    "\n",
    "+ 模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，通过这种调整，使得神经网络的中间输出值更加稳定。\n",
    "\n",
    "+ 对全连接层和卷积层做批量归一化的方法有所不同：\n",
    "\n",
    "**1.对全连接层做批量归一化**\n",
    "\n",
    "+ 批量归一化层位于全连接层的仿射变换和激活函数之之间。\n",
    "\n",
    "使用批量归一化的全连接层的输出为：\n",
    "\n",
    "$$\n",
    "\\phi(BN(x))\n",
    "$$\n",
    "\n",
    "其中，x为批量归一化的输入，由仿射变换$x=Wu+b$得到。$\\phi$为激活函数。\n",
    "\n",
    "如果一个个小批量有m个样本:\n",
    "\n",
    "$u^{(1)},u^{(2)},...,u^{(m)}$，\n",
    "\n",
    "则经过放射变化，得到:\n",
    "\n",
    "$x^{(1)},x^{(2)},...,x^{(m)}$\n",
    "\n",
    "这些就是批量归一化的输入，对于任意$x^{(i)}$，有\n",
    "$x^{(i)}\\in R^d, 1\\leq i \\leq m$，\n",
    "\n",
    "做批量归一化处理: \n",
    "\n",
    "$$\n",
    "\\hat{x}^{(i)}\\leftarrow \\frac{x^{(i)}-\\mu}{\\sqrt{(\\sigma ^2 + \\epsilon)}},\n",
    "\n",
    "$$\n",
    "\n",
    "其中，$\\mu$是这一批量样本的均值，$\\sigma$是这一批量样本的方差。计算如下：\n",
    "\n",
    "$$\n",
    "\\mu \\leftarrow=\\frac{1}{m}\\sum_{i=1}^{m}x^{(i)},\\\\\n",
    "\\sigma ^2 \\leftarrow \\frac{1}{m}\\sum_{i=1}^{m}(x^{(i)}-\\mu)^2,\n",
    "$$\n",
    "\n",
    "上述批量归一化，引入了两个可以学习的模型参数：拉伸参数$\\gamma$和偏移参数$\\beta$，这两个参数与$x^{(i)}$维数相同。\n",
    "\n",
    "于是得到:\n",
    "\n",
    "$y^{(i)}\\leftarrow \\gamma \\odot (\\hat{x}^{(i)}+\\beta)$\n",
    "\n",
    "其中，$\\odot$表示元素乘法。\n",
    "\n",
    "计算之后的$y_{i}$就是批量归一化的输出。\n",
    "\n",
    "Note:\n",
    "\n",
    "如果学习得到的$\\gamma=\\sqrt{(\\delta ^2 +\\epsilon)}$和$\\beta=\\mu$，则等价于没有做批量归一化。\n",
    "\n",
    "**2.对卷积层做批量归一化**\n",
    "\n",
    "+ 对于卷积层来说，批量归一化发生在卷积计算之后，激活函数之前，\n",
    "\n",
    "+ 如果卷积计算输出多个通道，则需要对每个通道的输出分别做批量归一化，\n",
    "\n",
    "+ 多通道时，每个通道都拥有独立的拉伸和偏移参数，\n",
    "\n",
    "+ 单通道时，如果卷积计算输出的宽和高是p\\*q，则需要对m\\*p\\*q个元素，同时做批量归一化。其中,m是小批量中样本的个数。\n",
    "\n",
    "**预测时的批量归一化**\n",
    "\n",
    "一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。\n",
    "\n",
    "和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "自定义实现批量归一化层："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch \n",
    "from torch import nn,optim \n",
    "import torch.nn.functional as F  \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import d2l_pytorch as d2l \n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(is_training,X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    # 区分当前模式\n",
    "    if not is_training:\n",
    "        # 预测模式下，直接使用出入的移动平均，由此来计算得到的均值和方差\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        # 训练模型\n",
    "        assert len(X.shape) in (2,4)\n",
    "        if len(X.shape)==2:\n",
    "            # 是全连接层，则计算特征维上的均值和方差\n",
    "            mean=X.mean(dim=0)\n",
    "            var=((X-mean)**2).mean(dim=0)\n",
    "        else:\n",
    "            # 是二维卷积层，则计算每个通道维上的均值和方差\n",
    "            # 保持X的形状，因为之后需要用到广播计算\n",
    "            mean=X.mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=0,keepdim=True).mean(dim=2,keepdim=True).mean(dim=3,keepdim=True)\n",
    "        # 训练模式下，使用小批量的均值和方差\n",
    "        X_hat=(X-mean)/torch.sqrt(var+eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean=momentum*moving_mean + (1.0-momentum)*mean\n",
    "        moving_var=momentum*moving_var + (1.0-momentum)*var \n",
    "    # 偏移和拉伸，得到BN输出\n",
    "    Y=gamma*X_hat+beta \n",
    "    return Y,moving_mean,moving_var"
   ]
  },
  {
   "source": [
    "自定义一个BatchNorm层，它保存参与求梯度和迭代的拉伸参数gamma和偏移参数beta。同时维护移动平均得到的均值和方差，用于模型预测。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features,num_dims):\n",
    "        # num_features为输入和输出的维度数,num_dims取值为2,4，用以判断是卷积层还是全连接层\n",
    "        super(BatchNorm,self).__init__()\n",
    "        if num_dims==2:\n",
    "            shape=(1,num_features)\n",
    "        else:\n",
    "            shape=(1,num_features,1,1)\n",
    "        #  # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1\n",
    "        self.gamma=nn.Parameter(torch.ones(shape))\n",
    "        self.beta=nn.Parameter(torch.zeros(shape))\n",
    "        # 不参与求梯度和迭代的变量，全在内存上初始化成0\n",
    "        self.moving_mean=torch.zeros(shape)\n",
    "        self.moving_var=torch.zeros(shape)\n",
    "    def forward(self,X):\n",
    "        # 如果X不在内存，则将moving_mean和moving_var移动到显存上\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean=self.moving_mean.to(X.device)\n",
    "            self.moving_var=self.moving_var.to(X.device)\n",
    "            # 保存更新过的moving_mean和moving_var\n",
    "        # Module实例的traning属性默认为true, 调用.eval()后设成false\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(self.training,X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=1e-5,momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "source": [
    "修改LeNet网络，添加批量归一化层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        # 卷积块包含2层：卷积层，激活函数，最大池化，卷积层，激活函数，最大池化\n",
    "        # in_channel=1,out_channle=6,kernelz-size=5\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            BatchNorm(6,num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5),\n",
    "            BatchNorm(16,num_dims=4),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(in_features=16*4*4,out_features=120),\n",
    "            BatchNorm(120,num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=120,out_features=84),\n",
    "            BatchNorm(84,num_dims=2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=84,out_features=10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        feature=self.conv(img)\n",
    "        output=self.fc(feature.view(img.shape[0],-1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "out shape: torch.Size([1, 16, 4, 4])\nout shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# 将输入的高和宽从224降到96，简化计算\n",
    "\n",
    "X=torch.rand(1,1,28,28)\n",
    "\n",
    "for blk in net.children():\n",
    "    X=blk(X)\n",
    "    print('out shape:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001,1   # 5\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training on  cpu\n",
      "epoch 0/1, iter 0/234, loss 2.335\n",
      "epoch 0/1, iter 1/234, loss 2.233\n",
      "epoch 0/1, iter 2/234, loss 2.148\n",
      "epoch 0/1, iter 3/234, loss 2.062\n",
      "epoch 0/1, iter 4/234, loss 2.004\n",
      "epoch 0/1, iter 5/234, loss 1.959\n",
      "epoch 0/1, iter 6/234, loss 1.908\n",
      "epoch 0/1, iter 7/234, loss 1.859\n",
      "epoch 0/1, iter 8/234, loss 1.848\n",
      "epoch 0/1, iter 9/234, loss 1.817\n",
      "epoch 0/1, iter 10/234, loss 1.817\n",
      "epoch 0/1, iter 11/234, loss 1.803\n",
      "epoch 0/1, iter 12/234, loss 1.741\n",
      "epoch 0/1, iter 13/234, loss 1.744\n",
      "epoch 0/1, iter 14/234, loss 1.745\n",
      "epoch 0/1, iter 15/234, loss 1.718\n",
      "epoch 0/1, iter 16/234, loss 1.687\n",
      "epoch 0/1, iter 17/234, loss 1.712\n",
      "epoch 0/1, iter 18/234, loss 1.649\n",
      "epoch 0/1, iter 19/234, loss 1.639\n",
      "epoch 0/1, iter 20/234, loss 1.627\n",
      "epoch 0/1, iter 21/234, loss 1.639\n",
      "epoch 0/1, iter 22/234, loss 1.623\n",
      "epoch 0/1, iter 23/234, loss 1.618\n",
      "epoch 0/1, iter 24/234, loss 1.599\n",
      "epoch 0/1, iter 25/234, loss 1.579\n",
      "epoch 0/1, iter 26/234, loss 1.590\n",
      "epoch 0/1, iter 27/234, loss 1.566\n",
      "epoch 0/1, iter 28/234, loss 1.577\n",
      "epoch 0/1, iter 29/234, loss 1.512\n",
      "epoch 0/1, iter 30/234, loss 1.514\n",
      "epoch 0/1, iter 31/234, loss 1.531\n",
      "epoch 0/1, iter 32/234, loss 1.484\n",
      "epoch 0/1, iter 33/234, loss 1.494\n",
      "epoch 0/1, iter 34/234, loss 1.476\n",
      "epoch 0/1, iter 35/234, loss 1.464\n",
      "epoch 0/1, iter 36/234, loss 1.468\n",
      "epoch 0/1, iter 37/234, loss 1.487\n",
      "epoch 0/1, iter 38/234, loss 1.449\n",
      "epoch 0/1, iter 39/234, loss 1.440\n",
      "epoch 0/1, iter 40/234, loss 1.423\n",
      "epoch 0/1, iter 41/234, loss 1.403\n",
      "epoch 0/1, iter 42/234, loss 1.414\n",
      "epoch 0/1, iter 43/234, loss 1.412\n",
      "epoch 0/1, iter 44/234, loss 1.373\n",
      "epoch 0/1, iter 45/234, loss 1.347\n",
      "epoch 0/1, iter 46/234, loss 1.380\n",
      "epoch 0/1, iter 47/234, loss 1.367\n",
      "epoch 0/1, iter 48/234, loss 1.358\n",
      "epoch 0/1, iter 49/234, loss 1.334\n",
      "epoch 0/1, iter 50/234, loss 1.352\n",
      "epoch 0/1, iter 51/234, loss 1.322\n",
      "epoch 0/1, iter 52/234, loss 1.298\n",
      "epoch 0/1, iter 53/234, loss 1.323\n",
      "epoch 0/1, iter 54/234, loss 1.319\n",
      "epoch 0/1, iter 55/234, loss 1.256\n",
      "epoch 0/1, iter 56/234, loss 1.304\n",
      "epoch 0/1, iter 57/234, loss 1.314\n",
      "epoch 0/1, iter 58/234, loss 1.272\n",
      "epoch 0/1, iter 59/234, loss 1.252\n",
      "epoch 0/1, iter 60/234, loss 1.258\n",
      "epoch 0/1, iter 61/234, loss 1.210\n",
      "epoch 0/1, iter 62/234, loss 1.248\n",
      "epoch 0/1, iter 63/234, loss 1.210\n",
      "epoch 0/1, iter 64/234, loss 1.220\n",
      "epoch 0/1, iter 65/234, loss 1.232\n",
      "epoch 0/1, iter 66/234, loss 1.196\n",
      "epoch 0/1, iter 67/234, loss 1.147\n",
      "epoch 0/1, iter 68/234, loss 1.197\n",
      "epoch 0/1, iter 69/234, loss 1.159\n",
      "epoch 0/1, iter 70/234, loss 1.153\n",
      "epoch 0/1, iter 71/234, loss 1.135\n",
      "epoch 0/1, iter 72/234, loss 1.161\n",
      "epoch 0/1, iter 73/234, loss 1.114\n",
      "epoch 0/1, iter 74/234, loss 1.130\n",
      "epoch 0/1, iter 75/234, loss 1.152\n",
      "epoch 0/1, iter 76/234, loss 1.158\n",
      "epoch 0/1, iter 77/234, loss 1.129\n",
      "epoch 0/1, iter 78/234, loss 1.115\n",
      "epoch 0/1, iter 79/234, loss 1.124\n",
      "epoch 0/1, iter 80/234, loss 1.074\n",
      "epoch 0/1, iter 81/234, loss 1.111\n",
      "epoch 0/1, iter 82/234, loss 1.118\n",
      "epoch 0/1, iter 83/234, loss 1.070\n",
      "epoch 0/1, iter 84/234, loss 1.065\n",
      "epoch 0/1, iter 85/234, loss 1.061\n",
      "epoch 0/1, iter 86/234, loss 1.067\n",
      "epoch 0/1, iter 87/234, loss 1.032\n",
      "epoch 0/1, iter 88/234, loss 1.068\n",
      "epoch 0/1, iter 89/234, loss 1.038\n",
      "epoch 0/1, iter 90/234, loss 1.065\n",
      "epoch 0/1, iter 91/234, loss 1.030\n",
      "epoch 0/1, iter 92/234, loss 0.989\n",
      "epoch 0/1, iter 93/234, loss 1.021\n",
      "epoch 0/1, iter 94/234, loss 1.051\n",
      "epoch 0/1, iter 95/234, loss 1.039\n",
      "epoch 0/1, iter 96/234, loss 1.004\n",
      "epoch 0/1, iter 97/234, loss 1.015\n",
      "epoch 0/1, iter 98/234, loss 1.006\n",
      "epoch 0/1, iter 99/234, loss 0.945\n",
      "epoch 0/1, iter 100/234, loss 0.989\n",
      "epoch 0/1, iter 101/234, loss 0.911\n",
      "epoch 0/1, iter 102/234, loss 1.056\n",
      "epoch 0/1, iter 103/234, loss 0.985\n",
      "epoch 0/1, iter 104/234, loss 0.994\n",
      "epoch 0/1, iter 105/234, loss 0.947\n",
      "epoch 0/1, iter 106/234, loss 0.930\n",
      "epoch 0/1, iter 107/234, loss 0.954\n",
      "epoch 0/1, iter 108/234, loss 0.954\n",
      "epoch 0/1, iter 109/234, loss 0.995\n",
      "epoch 0/1, iter 110/234, loss 0.897\n",
      "epoch 0/1, iter 111/234, loss 0.933\n",
      "epoch 0/1, iter 112/234, loss 0.884\n",
      "epoch 0/1, iter 113/234, loss 0.883\n",
      "epoch 0/1, iter 114/234, loss 0.924\n",
      "epoch 0/1, iter 115/234, loss 0.928\n",
      "epoch 0/1, iter 116/234, loss 0.911\n",
      "epoch 0/1, iter 117/234, loss 0.889\n",
      "epoch 0/1, iter 118/234, loss 0.892\n",
      "epoch 0/1, iter 119/234, loss 0.882\n",
      "epoch 0/1, iter 120/234, loss 0.857\n",
      "epoch 0/1, iter 121/234, loss 0.895\n",
      "epoch 0/1, iter 122/234, loss 0.879\n",
      "epoch 0/1, iter 123/234, loss 0.845\n",
      "epoch 0/1, iter 124/234, loss 0.855\n",
      "epoch 0/1, iter 125/234, loss 0.811\n",
      "epoch 0/1, iter 126/234, loss 0.846\n",
      "epoch 0/1, iter 127/234, loss 0.835\n",
      "epoch 0/1, iter 128/234, loss 0.810\n",
      "epoch 0/1, iter 129/234, loss 0.890\n",
      "epoch 0/1, iter 130/234, loss 0.839\n",
      "epoch 0/1, iter 131/234, loss 0.818\n",
      "epoch 0/1, iter 132/234, loss 0.755\n",
      "epoch 0/1, iter 133/234, loss 0.880\n",
      "epoch 0/1, iter 134/234, loss 0.804\n",
      "epoch 0/1, iter 135/234, loss 0.841\n",
      "epoch 0/1, iter 136/234, loss 0.779\n",
      "epoch 0/1, iter 137/234, loss 0.851\n",
      "epoch 0/1, iter 138/234, loss 0.776\n",
      "epoch 0/1, iter 139/234, loss 0.850\n",
      "epoch 0/1, iter 140/234, loss 0.774\n",
      "epoch 0/1, iter 141/234, loss 0.752\n",
      "epoch 0/1, iter 142/234, loss 0.779\n",
      "epoch 0/1, iter 143/234, loss 0.771\n",
      "epoch 0/1, iter 144/234, loss 0.733\n",
      "epoch 0/1, iter 145/234, loss 0.803\n",
      "epoch 0/1, iter 146/234, loss 0.755\n",
      "epoch 0/1, iter 147/234, loss 0.815\n",
      "epoch 0/1, iter 148/234, loss 0.767\n",
      "epoch 0/1, iter 149/234, loss 0.762\n",
      "epoch 0/1, iter 150/234, loss 0.733\n",
      "epoch 0/1, iter 151/234, loss 0.781\n",
      "epoch 0/1, iter 152/234, loss 0.789\n",
      "epoch 0/1, iter 153/234, loss 0.713\n",
      "epoch 0/1, iter 154/234, loss 0.718\n",
      "epoch 0/1, iter 155/234, loss 0.757\n",
      "epoch 0/1, iter 156/234, loss 0.760\n",
      "epoch 0/1, iter 157/234, loss 0.741\n",
      "epoch 0/1, iter 158/234, loss 0.737\n",
      "epoch 0/1, iter 159/234, loss 0.753\n",
      "epoch 0/1, iter 160/234, loss 0.730\n",
      "epoch 0/1, iter 161/234, loss 0.725\n",
      "epoch 0/1, iter 162/234, loss 0.771\n",
      "epoch 0/1, iter 163/234, loss 0.680\n",
      "epoch 0/1, iter 164/234, loss 0.751\n",
      "epoch 0/1, iter 165/234, loss 0.660\n",
      "epoch 0/1, iter 166/234, loss 0.680\n",
      "epoch 0/1, iter 167/234, loss 0.702\n",
      "epoch 0/1, iter 168/234, loss 0.664\n",
      "epoch 0/1, iter 169/234, loss 0.691\n",
      "epoch 0/1, iter 170/234, loss 0.736\n",
      "epoch 0/1, iter 171/234, loss 0.684\n",
      "epoch 0/1, iter 172/234, loss 0.689\n",
      "epoch 0/1, iter 173/234, loss 0.723\n",
      "epoch 0/1, iter 174/234, loss 0.677\n",
      "epoch 0/1, iter 175/234, loss 0.734\n",
      "epoch 0/1, iter 176/234, loss 0.656\n",
      "epoch 0/1, iter 177/234, loss 0.643\n",
      "epoch 0/1, iter 178/234, loss 0.694\n",
      "epoch 0/1, iter 179/234, loss 0.701\n",
      "epoch 0/1, iter 180/234, loss 0.599\n",
      "epoch 0/1, iter 181/234, loss 0.673\n",
      "epoch 0/1, iter 182/234, loss 0.676\n",
      "epoch 0/1, iter 183/234, loss 0.655\n",
      "epoch 0/1, iter 184/234, loss 0.676\n",
      "epoch 0/1, iter 185/234, loss 0.648\n",
      "epoch 0/1, iter 186/234, loss 0.648\n",
      "epoch 0/1, iter 187/234, loss 0.654\n",
      "epoch 0/1, iter 188/234, loss 0.685\n",
      "epoch 0/1, iter 189/234, loss 0.670\n",
      "epoch 0/1, iter 190/234, loss 0.624\n",
      "epoch 0/1, iter 191/234, loss 0.643\n",
      "epoch 0/1, iter 192/234, loss 0.659\n",
      "epoch 0/1, iter 193/234, loss 0.696\n",
      "epoch 0/1, iter 194/234, loss 0.628\n",
      "epoch 0/1, iter 195/234, loss 0.661\n",
      "epoch 0/1, iter 196/234, loss 0.648\n",
      "epoch 0/1, iter 197/234, loss 0.599\n",
      "epoch 0/1, iter 198/234, loss 0.600\n",
      "epoch 0/1, iter 199/234, loss 0.591\n",
      "epoch 0/1, iter 200/234, loss 0.613\n",
      "epoch 0/1, iter 201/234, loss 0.609\n",
      "epoch 0/1, iter 202/234, loss 0.702\n",
      "epoch 0/1, iter 203/234, loss 0.720\n",
      "epoch 0/1, iter 204/234, loss 0.584\n",
      "epoch 0/1, iter 205/234, loss 0.656\n",
      "epoch 0/1, iter 206/234, loss 0.661\n",
      "epoch 0/1, iter 207/234, loss 0.649\n",
      "epoch 0/1, iter 208/234, loss 0.675\n",
      "epoch 0/1, iter 209/234, loss 0.638\n",
      "epoch 0/1, iter 210/234, loss 0.613\n",
      "epoch 0/1, iter 211/234, loss 0.631\n",
      "epoch 0/1, iter 212/234, loss 0.706\n",
      "epoch 0/1, iter 213/234, loss 0.589\n",
      "epoch 0/1, iter 214/234, loss 0.684\n",
      "epoch 0/1, iter 215/234, loss 0.655\n",
      "epoch 0/1, iter 216/234, loss 0.572\n",
      "epoch 0/1, iter 217/234, loss 0.577\n",
      "epoch 0/1, iter 218/234, loss 0.646\n",
      "epoch 0/1, iter 219/234, loss 0.604\n",
      "epoch 0/1, iter 220/234, loss 0.607\n",
      "epoch 0/1, iter 221/234, loss 0.536\n",
      "epoch 0/1, iter 222/234, loss 0.606\n",
      "epoch 0/1, iter 223/234, loss 0.647\n",
      "epoch 0/1, iter 224/234, loss 0.653\n",
      "epoch 0/1, iter 225/234, loss 0.617\n",
      "epoch 0/1, iter 226/234, loss 0.587\n",
      "epoch 0/1, iter 227/234, loss 0.667\n",
      "epoch 0/1, iter 228/234, loss 0.605\n",
      "epoch 0/1, iter 229/234, loss 0.584\n",
      "epoch 0/1, iter 230/234, loss 0.606\n",
      "epoch 0/1, iter 231/234, loss 0.602\n",
      "epoch 0/1, iter 232/234, loss 0.575\n",
      "epoch 0/1, iter 233/234, loss 0.623\n",
      "epoch 0/1, iter 234/234, loss 0.575\n",
      "epoch 1, loss 1.0159, train acc 0.774, test acc 0.807, time 38.1 sec\n"
     ]
    }
   ],
   "source": [
    "d2l.train_ch05(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\n"
   ]
  },
  {
   "source": [
    "## BatchNorm简洁实现\n",
    "\n",
    "使用nn.BatchNorm1d()和nn.BatchNorm2d()类，分别用于全连接层和卷积层的批量归一化。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet2,self).__init__()\n",
    "        # 卷积块包含2层：卷积层，激活函数，最大池化，卷积层，激活函数，最大池化\n",
    "        # in_channel=1,out_channle=6,kernelz-size=5\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv2d(1,6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6,16,5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            d2l.FlattenLayer(),\n",
    "            nn.Linear(in_features=16*4*4,out_features=120),\n",
    "            nn.BatchNorm1d(120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.BatchNorm1d(84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84,10)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "        feature=self.conv(img)\n",
    "        output=self.fc(feature.view(img.shape[0],-1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "out shape: torch.Size([1, 16, 4, 4])\nout shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "net2=LeNet2()\n",
    "X=torch.rand(1,1,28,28)\n",
    "\n",
    "for blk in net.children():\n",
    "    X=blk(X)\n",
    "    print('out shape:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)\n",
    "\n",
    "lr, num_epochs = 0.001, 1\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "d2l.train_ch05(net2, train_iter, test_iter, batch_size, optimizer, device, num_epochs)"
   ]
  },
  {
   "source": [
    "**小结**\n",
    "\n",
    "模型训练时，批量归一化利用小批量的均值和标准差，不断调整神经网络的输出，从而使得神经网络的中间输出值更加稳定，\n",
    "\n",
    "全连接层和卷积层的批量归一化各有不同，\n",
    "\n",
    "批量归一化层和丢弃层一样，训练和验证模型下，结果不同，\n",
    "\n",
    "使用nn.BatchNorm(num_features)简洁实现。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}