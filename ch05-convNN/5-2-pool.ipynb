{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.3 多输入通道和多输出通道\n",
    "\n",
    "当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算\n",
    "\n",
    "\n",
    "多输入通道的互相关运算：对每个通道互相关运算，然后使用add_n函数加和，加和之后，输出的通道数就为1。\n",
    "> 卷积运算中，将每个输入通道与卷积核所有通道进行互相相关运算，将结果进行加和，就得到单通道输出。将所有的单通道输出合并，就得到多输出通道。\n",
    "\n",
    "代码："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2l_pytorch as d2l"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "    # 沿着X和K的第0维(通道维)卷积，再相加\n",
    "    # 对第1个通道，进行卷积运算\n",
    "    res=d2l.corr2d(X[0,:,:],K[0,:,:])\n",
    "    # 对剩余通道进行卷积运算\n",
    "    for i in range(1,X.shape[0]):\n",
    "        res+=d2l.corr2d(X[i,:,:],K[i,:,:])\n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X: torch.Size([2, 3, 3])\nK: torch.Size([2, 2, 2])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# test\n",
    "X = torch.tensor([\n",
    "    [\n",
    "     [0, 1, 2], \n",
    "     [3, 4, 5], \n",
    "     [6, 7, 8]\n",
    "    ],\n",
    "    [\n",
    "     [1, 2, 3], \n",
    "     [4, 5, 6], \n",
    "     [7, 8, 9]\n",
    "    ]\n",
    "])\n",
    "\n",
    "K = torch.tensor([\n",
    "    [   [0, 1], \n",
    "        [2, 3]\n",
    "    ], \n",
    "    [\n",
    "        [1, 2], \n",
    "        [3, 4]\n",
    "    ]\n",
    "])\n",
    "\n",
    "print(\"X:\",X.shape)\n",
    "print(\"K:\",K.shape)\n",
    "\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "source": [
    "## 多输出通道\n",
    "\n",
    "代码："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_out(X,K):\n",
    "    # 对K的所有维进行遍历，每次遍历：该维与X做互相关运算，这样就能得到一个通道的输出了\n",
    "    # 最后将结果合并在一起\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K: torch.Size([3, 2, 2, 2])\nX: torch.Size([2, 3, 3])\ntensor([[[[0, 1],\n          [2, 3]],\n\n         [[1, 2],\n          [3, 4]]],\n\n\n        [[[1, 2],\n          [3, 4]],\n\n         [[2, 3],\n          [4, 5]]],\n\n\n        [[[2, 3],\n          [4, 5]],\n\n         [[3, 4],\n          [5, 6]]]])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# 2*2*2的卷积核，最左边的2表示通道数\n",
    "K = torch.tensor([\n",
    "    [   [0, 1], \n",
    "        [2, 3]\n",
    "    ], \n",
    "    [\n",
    "        [1, 2], \n",
    "        [3, 4]\n",
    "    ]\n",
    "])\n",
    "# 2*2*2,2个通道数,一共3个批量\n",
    "K=torch.stack([K,K+1,K+2])\n",
    "print(\"K:\",K.shape)\n",
    "print(\"X:\",X.shape)\n",
    "print(K)\n",
    "# batch_size, channel, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Y: torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "Y=corr2d_multi_out(X,K)\n",
    "print('Y:',Y.shape)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "source": [
    "## 1\\*1的卷积层\n",
    "\n",
    "卷积窗口长和宽都为1的多通道卷积层称为1*1卷积层，\n",
    "\n",
    "该层输入和输出具有相同的高和宽，输出中的每个元素来自输入中在高和宽上相同位置的元素在不同通道之间的按权重累加。\n",
    "\n",
    "假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1×1卷积层的作用与全连接层等价。\n",
    "\n",
    "经常当作保持高和宽维度形状不变的全连接层使用。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_out_1(X,K):\n",
    "    c_i,h,w=X.shape\n",
    "    c_o=K.shape[0]\n",
    "    # 改变输入的维度\n",
    "    X=X.view(c_i,h*w)\n",
    "    # 改变卷积层的维度\n",
    "    K=K.view(c_o,c_i)\n",
    "    # 输出\n",
    "    return torch.mm(K,X).view(c_o,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# test\n",
    "X = torch.rand(3, 3, 3)\n",
    "K = torch.rand(2, 3, 1, 1)\n",
    "\n",
    "Y1=corr2d_multi_out(X,K)\n",
    "Y2=corr2d_multi_out_1(X,K)\n",
    "\n",
    "(Y1-Y2).norm().item()<1e-6"
   ]
  },
  {
   "source": [
    "# 5.4 池化层\n",
    "\n",
    "池化层的提出，是为了缓解卷积层对于位置的过度敏感性。\n",
    "\n",
    "分类：\n",
    "\n",
    "+ 最大池化\n",
    "\n",
    "+ 平均池化\n",
    "\n",
    "代码："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X,pool_size,mode='max'):\n",
    "    X=X.float()\n",
    "    # 池化层的高和宽\n",
    "    p_h,p_w=pool_size\n",
    "    # 输出，尺寸\n",
    "    Y=torch.zeros(X.shape[0]-p_h+1,X.shape[1]-p_w+1)\n",
    "    # 行\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode=='max':\n",
    "                Y[i,j]=X[i:i+p_h,j:j+p_w].max()\n",
    "            elif mode=='avg':\n",
    "                Y[i,j]=X[i:i+p_h,j:j+p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# test\n",
    "X=torch.tensor([\n",
    "    [0,1,2],\n",
    "    [3,4,5],\n",
    "    [6,7,8]\n",
    "])\n",
    "pool2d(X,(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "pool2d(X,(2,2),'avg')"
   ]
  },
  {
   "source": [
    "## 池化层中使用填充和步幅\n",
    "\n",
    "简洁实现：使用torch.nn.MaxPool2d()\n",
    "\n",
    "代码："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X=torch.arange(16,dtype=torch.float)\n",
    "X=X.view(1,1,4,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[10.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 使用形状为3*3的池化窗口，步幅默认为3\n",
    "pool2d=torch.nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# 指定步幅和填充\n",
    "\n",
    "pool2d=torch.nn.MaxPool2d(3,padding=1,stride=2)\n",
    "pool2d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 1.,  3.],\n",
       "          [ 9., 11.],\n",
       "          [13., 15.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "pool2d = torch.nn.MaxPool2d((2, 4), padding=(1, 2), stride=(2, 3))\n",
    "pool2d(X)"
   ]
  },
  {
   "source": [
    "## 多通道的池化层\n",
    "\n",
    "在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.,  3.],\n",
       "          [ 4.,  5.,  6.,  7.],\n",
       "          [ 8.,  9., 10., 11.],\n",
       "          [12., 13., 14., 15.]],\n",
       "\n",
       "         [[ 1.,  2.,  3.,  4.],\n",
       "          [ 5.,  6.,  7.,  8.],\n",
       "          [ 9., 10., 11., 12.],\n",
       "          [13., 14., 15., 16.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "# X2=torch.stack([X,X+1]) \n",
    "# 不等价于\n",
    "X2=torch.cat((X,X+1),dim=1)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 4, 4])\ntorch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2, 1, 4, 4])\ntensor([[[[[ 0.,  1.,  2.,  3.],\n           [ 4.,  5.,  6.,  7.],\n           [ 8.,  9., 10., 11.],\n           [12., 13., 14., 15.]]],\n\n\n         [[[ 1.,  2.,  3.,  4.],\n           [ 5.,  6.,  7.,  8.],\n           [ 9., 10., 11., 12.],\n           [13., 14., 15., 16.]]]]])\n"
     ]
    }
   ],
   "source": [
    "X3=torch.stack([X,X+1],dim=1)\n",
    "print(X3.shape)\n",
    "print(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[ 5.,  7.],\n",
       "          [13., 15.]],\n",
       "\n",
       "         [[ 6.,  8.],\n",
       "          [14., 16.]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "pool2d = torch.nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}