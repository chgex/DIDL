{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.6 AlexNet\n",
    "\n",
    "计算机视觉流程中真正重要的是数据和特征。也就是说，使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "import torchvision\n",
    "import d2l_pytorch as d2l \n",
    "import time\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            # layer 2\n",
    "            nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,groups=2,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            # layer 3\n",
    "            nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # layer 4\n",
    "            nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # layer 5\n",
    "            nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            # layer 6\n",
    "            nn.Linear(in_features=6*6*256,out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            # layer 7\n",
    "            nn.Linear(in_features=4096,out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            # layer 8\n",
    "            nn.Linear(in_features=4096,out_features=num_classes)\n",
    "        )\n",
    "        def forward(self,img):\n",
    "            feature=self.conv(img)\n",
    "            # output=self.fc(feature.view(img.shape[0],-1))\n",
    "            output=self.fc(feature.view(-1,6*6*256))\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AlexNet(\n  (conv): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=2)\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=9216, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# print net\n",
    "net=AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "## 读取数据\n",
    "\n",
    "读取数据的时候我们额外做了一步将图像高和宽扩大到AlexNet使用的图像高和宽224。\n",
    "\n",
    "这个可以通过torchvision.transforms.Resize实例来实现。\n",
    "\n",
    "也就是说，我们在ToTensor实例前使用Resize实例，然后使用Compose实例来将这两个变换串联以方便调用。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建一个列表：将多个图像变换实例集合到一起\n",
    "\n",
    "resize=227\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=resize),\n",
    "    transforms.ToTensor(),\n",
    "  \t# Normalize 这8个值是针对 CIFAR-10 这个数据集算出来的，对于其他数据集不适用\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../datasets/cifar-10-python.tar.gz\n",
      "100.0%Extracting ../datasets/cifar-10-python.tar.gz to ../datasets/\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/', \n",
    "    train=True, \n",
    "    download=False, \n",
    "    transform=transform)\n",
    "\n",
    "test_dataset= torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/', \n",
    "    train=False,\n",
    "    download=False, \n",
    "    transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_iter=torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4)\n",
    "test_iter=torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4)\n"
   ]
  }
 ]
}