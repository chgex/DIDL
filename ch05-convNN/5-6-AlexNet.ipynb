{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.6 AlexNet\n",
    "\n",
    "计算机视觉流程中真正重要的是数据和特征。也就是说，使用较干净的数据集和较有效的特征甚至比机器学习模型的选择对图像分类结果的影响更大。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "import torchvision\n",
    "# import d2l_pytorch as d2l \n",
    "import time\n",
    "sys.path.append('..')\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.conv=nn.Sequential(\n",
    "            # layer 1\n",
    "            nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11,stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            # layer 2\n",
    "            nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,groups=2,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            # layer 3\n",
    "            nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # layer 4\n",
    "            nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # layer 5\n",
    "            nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            # layer 6\n",
    "            nn.Linear(in_features=6*6*256,out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            # layer 7\n",
    "            nn.Linear(in_features=4096,out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            # layer 8\n",
    "            nn.Linear(in_features=4096,out_features=num_classes)\n",
    "        )\n",
    "    def forward(self,img):\n",
    "            feature=self.conv(img)\n",
    "            # output=self.fc(feature.view(img.shape[0],-1))\n",
    "            output=self.fc(feature.view(-1,6*6*256))\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AlexNet(\n  (conv): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=2)\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=9216, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# print net\n",
    "net=AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "source": [
    "## 读取数据\n",
    "\n",
    "读取数据的时候我们额外做了一步将图像高和宽扩大到AlexNet使用的图像高和宽224。\n",
    "\n",
    "这个可以通过torchvision.transforms.Resize实例来实现。\n",
    "\n",
    "也就是说，我们在ToTensor实例前使用Resize实例，然后使用Compose实例来将这两个变换串联以方便调用。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像预处理\n",
    "\n",
    "# 构建一个列表：将多个图像变换实例集合到一起\n",
    "\n",
    "resize=227\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=resize),\n",
    "    transforms.ToTensor(),\n",
    "  \t# Normalize 这8个值是针对 CIFAR-10 这个数据集算出来的，对于其他数据集不适用\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "\n",
    "# CIFAR10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/', \n",
    "    train=True, \n",
    "    download=False, \n",
    "    transform=transform)\n",
    "\n",
    "test_dataset= torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/', \n",
    "    train=False,\n",
    "    download=False, \n",
    "    transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "\n",
    "# data loader\n",
    "train_iter=torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=4)\n",
    "test_iter=torch.utils.data.DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_iter)   # 391\n",
    "\n",
    "def evaluate_accuracy_ch05(data_iter,net,device=None):\n",
    "    # gpu\n",
    "    if device is None and isinstance(net,torch.nn.Module):\n",
    "        # 如果没有指定device，则使用net的device\n",
    "        device=list(net.parameters())[0].device\n",
    "    # 准确率，总数\n",
    "    acc_sum,n=0.0,0\n",
    "    # with torch.no_grad： disables tracking of gradients in autograd. \n",
    "    # model.eval()： changes the forward() behaviour of the module it is called upon.\n",
    "    with torch.no_grad():\n",
    "        for X,y in data_iter:\n",
    "            if isinstance(net,torch.nn.Module):\n",
    "                # 评估模式，该模式会关闭dropout\n",
    "                net.eval()\n",
    "                # torch.argmax(input, dim, keepdim=False) → LongTensor返回指定维度的最大值的索引。\n",
    "                acc_sum+=( net(X.to(device)).argmax(dim=1) == y.to(device) ).float().sum().cpu().item()\n",
    "            else: # 无GPU\n",
    "                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数\n",
    "                    # 将is_training设置成False\n",
    "                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() \n",
    "                else:\n",
    "                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() \n",
    "            n+=y.shape[0]\n",
    "    return acc_sum/n\n",
    "\n",
    "def train_ch05(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs):\n",
    "    net=net.to(device)\n",
    "    print('training on ',device)\n",
    "    # 损失函数，使用交叉熵损失函数\n",
    "    loss=torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,batch_count,start=0.0,0.0,0,0,time.time()\n",
    "        for i,(X,y) in enumerate(train_iter):\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_hat=net(X)\n",
    "            # 计算损失\n",
    "            l=loss(y_hat,y)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 反向传播\n",
    "            l.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            \n",
    "            print('epoch %d/%d, iter %d/391, loss %.3f' % (epoch,num_epochs,i,l.cpu().item()))\n",
    "\n",
    "\n",
    "            # 更新损失和正确率\n",
    "            train_l_sum+=l.cpu().item()\n",
    "            train_acc_sum+=(y_hat.argmax(dim=1) == y ).sum().cpu().item()\n",
    "            n+=y.shape[0]\n",
    "            batch_count+=1\n",
    "        # 测试集上的正确率\n",
    "        test_acc=evaluate_accuracy_ch05(test_iter,net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "        %(epoch+1,train_l_sum/batch_count,train_acc_sum/n,test_acc,time.time()-start))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "# learning rate, epoch\n",
    "lr,num_epochs=0.001,1\n",
    "# optimizer\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training on  cpu\n",
      "epoch 0/1, iter 0/391, loss 4.177\n",
      "epoch 0/1, iter 1/391, loss 2.314\n",
      "epoch 0/1, iter 2/391, loss 2.302\n",
      "epoch 0/1, iter 3/391, loss 2.312\n",
      "epoch 0/1, iter 4/391, loss 2.298\n",
      "epoch 0/1, iter 5/391, loss 2.299\n",
      "epoch 0/1, iter 6/391, loss 2.309\n",
      "epoch 0/1, iter 7/391, loss 2.307\n",
      "epoch 0/1, iter 8/391, loss 2.309\n",
      "epoch 0/1, iter 9/391, loss 2.294\n",
      "epoch 0/1, iter 10/391, loss 2.298\n",
      "epoch 0/1, iter 11/391, loss 2.308\n",
      "epoch 0/1, iter 12/391, loss 2.298\n",
      "epoch 0/1, iter 13/391, loss 2.308\n",
      "epoch 0/1, iter 14/391, loss 2.288\n",
      "epoch 0/1, iter 15/391, loss 2.301\n",
      "epoch 0/1, iter 16/391, loss 2.298\n",
      "epoch 0/1, iter 17/391, loss 2.317\n",
      "epoch 0/1, iter 18/391, loss 2.291\n",
      "epoch 0/1, iter 19/391, loss 2.289\n",
      "epoch 0/1, iter 20/391, loss 2.339\n",
      "epoch 0/1, iter 21/391, loss 2.266\n",
      "epoch 0/1, iter 22/391, loss 2.228\n",
      "epoch 0/1, iter 23/391, loss 2.300\n",
      "epoch 0/1, iter 24/391, loss 2.217\n",
      "epoch 0/1, iter 25/391, loss 2.192\n",
      "epoch 0/1, iter 26/391, loss 2.231\n",
      "epoch 0/1, iter 27/391, loss 2.162\n",
      "epoch 0/1, iter 28/391, loss 2.269\n",
      "epoch 0/1, iter 29/391, loss 2.204\n",
      "epoch 0/1, iter 30/391, loss 2.163\n",
      "epoch 0/1, iter 31/391, loss 2.146\n",
      "epoch 0/1, iter 32/391, loss 2.087\n",
      "epoch 0/1, iter 33/391, loss 2.191\n",
      "epoch 0/1, iter 34/391, loss 2.220\n",
      "epoch 0/1, iter 35/391, loss 2.200\n",
      "epoch 0/1, iter 36/391, loss 2.191\n",
      "epoch 0/1, iter 37/391, loss 2.123\n",
      "epoch 0/1, iter 38/391, loss 2.270\n",
      "epoch 0/1, iter 39/391, loss 2.046\n",
      "epoch 0/1, iter 40/391, loss 2.126\n",
      "epoch 0/1, iter 41/391, loss 2.177\n",
      "epoch 0/1, iter 42/391, loss 2.157\n",
      "epoch 0/1, iter 43/391, loss 2.127\n",
      "epoch 0/1, iter 44/391, loss 2.096\n",
      "epoch 0/1, iter 45/391, loss 2.085\n",
      "epoch 0/1, iter 46/391, loss 2.166\n",
      "epoch 0/1, iter 47/391, loss 2.167\n",
      "epoch 0/1, iter 48/391, loss 2.125\n",
      "epoch 0/1, iter 49/391, loss 2.072\n",
      "epoch 0/1, iter 50/391, loss 2.018\n",
      "epoch 0/1, iter 51/391, loss 2.149\n",
      "epoch 0/1, iter 52/391, loss 2.061\n",
      "epoch 0/1, iter 53/391, loss 2.070\n",
      "epoch 0/1, iter 54/391, loss 2.075\n",
      "epoch 0/1, iter 55/391, loss 2.040\n",
      "epoch 0/1, iter 56/391, loss 1.945\n",
      "epoch 0/1, iter 57/391, loss 2.125\n",
      "epoch 0/1, iter 58/391, loss 1.924\n",
      "epoch 0/1, iter 59/391, loss 1.958\n",
      "epoch 0/1, iter 60/391, loss 1.958\n",
      "epoch 0/1, iter 61/391, loss 1.976\n",
      "epoch 0/1, iter 62/391, loss 2.036\n",
      "epoch 0/1, iter 63/391, loss 2.003\n",
      "epoch 0/1, iter 64/391, loss 1.968\n",
      "epoch 0/1, iter 65/391, loss 1.980\n",
      "epoch 0/1, iter 66/391, loss 2.022\n",
      "epoch 0/1, iter 67/391, loss 1.956\n",
      "epoch 0/1, iter 68/391, loss 1.859\n",
      "epoch 0/1, iter 69/391, loss 1.990\n",
      "epoch 0/1, iter 70/391, loss 1.962\n",
      "epoch 0/1, iter 71/391, loss 2.004\n",
      "epoch 0/1, iter 72/391, loss 2.099\n",
      "epoch 0/1, iter 73/391, loss 1.901\n",
      "epoch 0/1, iter 74/391, loss 1.958\n",
      "epoch 0/1, iter 75/391, loss 1.955\n",
      "epoch 0/1, iter 76/391, loss 2.027\n",
      "epoch 0/1, iter 77/391, loss 1.974\n",
      "epoch 0/1, iter 78/391, loss 1.842\n",
      "epoch 0/1, iter 79/391, loss 1.850\n",
      "epoch 0/1, iter 80/391, loss 1.967\n",
      "epoch 0/1, iter 81/391, loss 1.943\n",
      "epoch 0/1, iter 82/391, loss 1.870\n",
      "epoch 0/1, iter 83/391, loss 1.898\n",
      "epoch 0/1, iter 84/391, loss 1.988\n",
      "epoch 0/1, iter 85/391, loss 2.024\n",
      "epoch 0/1, iter 86/391, loss 1.881\n",
      "epoch 0/1, iter 87/391, loss 1.977\n",
      "epoch 0/1, iter 88/391, loss 2.070\n",
      "epoch 0/1, iter 89/391, loss 1.920\n",
      "epoch 0/1, iter 90/391, loss 2.002\n",
      "epoch 0/1, iter 91/391, loss 1.939\n",
      "epoch 0/1, iter 92/391, loss 1.927\n",
      "epoch 0/1, iter 93/391, loss 2.004\n",
      "epoch 0/1, iter 94/391, loss 1.871\n",
      "epoch 0/1, iter 95/391, loss 1.975\n",
      "epoch 0/1, iter 96/391, loss 1.897\n",
      "epoch 0/1, iter 97/391, loss 1.902\n",
      "epoch 0/1, iter 98/391, loss 1.977\n",
      "epoch 0/1, iter 99/391, loss 1.889\n",
      "epoch 0/1, iter 100/391, loss 1.817\n",
      "epoch 0/1, iter 101/391, loss 1.958\n",
      "epoch 0/1, iter 102/391, loss 1.914\n",
      "epoch 0/1, iter 103/391, loss 1.731\n",
      "epoch 0/1, iter 104/391, loss 1.899\n",
      "epoch 0/1, iter 105/391, loss 1.888\n",
      "epoch 0/1, iter 106/391, loss 1.790\n",
      "epoch 0/1, iter 107/391, loss 1.805\n",
      "epoch 0/1, iter 108/391, loss 1.931\n",
      "epoch 0/1, iter 109/391, loss 1.802\n",
      "epoch 0/1, iter 110/391, loss 1.809\n",
      "epoch 0/1, iter 111/391, loss 1.885\n",
      "epoch 0/1, iter 112/391, loss 1.891\n",
      "epoch 0/1, iter 113/391, loss 1.837\n",
      "epoch 0/1, iter 114/391, loss 1.709\n",
      "epoch 0/1, iter 115/391, loss 1.806\n",
      "epoch 0/1, iter 116/391, loss 1.898\n",
      "epoch 0/1, iter 117/391, loss 1.793\n",
      "epoch 0/1, iter 118/391, loss 1.765\n",
      "epoch 0/1, iter 119/391, loss 1.858\n",
      "epoch 0/1, iter 120/391, loss 1.788\n",
      "epoch 0/1, iter 121/391, loss 1.869\n",
      "epoch 0/1, iter 122/391, loss 1.767\n",
      "epoch 0/1, iter 123/391, loss 1.819\n",
      "epoch 0/1, iter 124/391, loss 1.854\n",
      "epoch 0/1, iter 125/391, loss 1.841\n",
      "epoch 0/1, iter 126/391, loss 1.749\n",
      "epoch 0/1, iter 127/391, loss 1.867\n",
      "epoch 0/1, iter 128/391, loss 1.733\n",
      "epoch 0/1, iter 129/391, loss 1.837\n",
      "epoch 0/1, iter 130/391, loss 1.829\n",
      "epoch 0/1, iter 131/391, loss 1.916\n",
      "epoch 0/1, iter 132/391, loss 1.707\n",
      "epoch 0/1, iter 133/391, loss 1.852\n",
      "epoch 0/1, iter 134/391, loss 1.728\n",
      "epoch 0/1, iter 135/391, loss 1.810\n",
      "epoch 0/1, iter 136/391, loss 1.672\n",
      "epoch 0/1, iter 137/391, loss 1.651\n",
      "epoch 0/1, iter 138/391, loss 1.696\n",
      "epoch 0/1, iter 139/391, loss 1.823\n",
      "epoch 0/1, iter 140/391, loss 1.686\n",
      "epoch 0/1, iter 141/391, loss 1.706\n",
      "epoch 0/1, iter 142/391, loss 1.685\n",
      "epoch 0/1, iter 143/391, loss 1.676\n",
      "epoch 0/1, iter 144/391, loss 1.561\n",
      "epoch 0/1, iter 145/391, loss 1.657\n",
      "epoch 0/1, iter 146/391, loss 1.711\n",
      "epoch 0/1, iter 147/391, loss 1.667\n",
      "epoch 0/1, iter 148/391, loss 1.823\n",
      "epoch 0/1, iter 149/391, loss 1.709\n",
      "epoch 0/1, iter 150/391, loss 1.779\n",
      "epoch 0/1, iter 151/391, loss 1.559\n",
      "epoch 0/1, iter 152/391, loss 1.739\n",
      "epoch 0/1, iter 153/391, loss 1.750\n",
      "epoch 0/1, iter 154/391, loss 1.527\n",
      "epoch 0/1, iter 155/391, loss 1.744\n",
      "epoch 0/1, iter 156/391, loss 1.688\n",
      "epoch 0/1, iter 157/391, loss 1.829\n",
      "epoch 0/1, iter 158/391, loss 1.586\n",
      "epoch 0/1, iter 159/391, loss 1.753\n",
      "epoch 0/1, iter 160/391, loss 1.764\n",
      "epoch 0/1, iter 161/391, loss 1.603\n",
      "epoch 0/1, iter 162/391, loss 1.635\n",
      "epoch 0/1, iter 163/391, loss 1.646\n",
      "epoch 0/1, iter 164/391, loss 1.695\n",
      "epoch 0/1, iter 165/391, loss 1.636\n",
      "epoch 0/1, iter 166/391, loss 1.600\n",
      "epoch 0/1, iter 167/391, loss 1.798\n",
      "epoch 0/1, iter 168/391, loss 1.499\n",
      "epoch 0/1, iter 169/391, loss 1.752\n",
      "epoch 0/1, iter 170/391, loss 1.649\n",
      "epoch 0/1, iter 171/391, loss 1.646\n",
      "epoch 0/1, iter 172/391, loss 1.620\n",
      "epoch 0/1, iter 173/391, loss 1.618\n",
      "epoch 0/1, iter 174/391, loss 1.578\n",
      "epoch 0/1, iter 175/391, loss 1.507\n",
      "epoch 0/1, iter 176/391, loss 1.652\n",
      "epoch 0/1, iter 177/391, loss 1.574\n",
      "epoch 0/1, iter 178/391, loss 1.589\n",
      "epoch 0/1, iter 179/391, loss 1.694\n",
      "epoch 0/1, iter 180/391, loss 1.677\n",
      "epoch 0/1, iter 181/391, loss 1.824\n",
      "epoch 0/1, iter 182/391, loss 1.721\n",
      "epoch 0/1, iter 183/391, loss 1.755\n",
      "epoch 0/1, iter 184/391, loss 1.686\n",
      "epoch 0/1, iter 185/391, loss 1.744\n",
      "epoch 0/1, iter 186/391, loss 1.655\n",
      "epoch 0/1, iter 187/391, loss 1.630\n",
      "epoch 0/1, iter 188/391, loss 1.639\n",
      "epoch 0/1, iter 189/391, loss 1.735\n",
      "epoch 0/1, iter 190/391, loss 1.600\n",
      "epoch 0/1, iter 191/391, loss 1.681\n",
      "epoch 0/1, iter 192/391, loss 1.683\n",
      "epoch 0/1, iter 193/391, loss 1.691\n",
      "epoch 0/1, iter 194/391, loss 1.572\n",
      "epoch 0/1, iter 195/391, loss 1.601\n",
      "epoch 0/1, iter 196/391, loss 1.603\n",
      "epoch 0/1, iter 197/391, loss 1.680\n",
      "epoch 0/1, iter 198/391, loss 1.585\n",
      "epoch 0/1, iter 199/391, loss 1.615\n",
      "epoch 0/1, iter 200/391, loss 1.604\n",
      "epoch 0/1, iter 201/391, loss 1.464\n",
      "epoch 0/1, iter 202/391, loss 1.546\n",
      "epoch 0/1, iter 203/391, loss 1.582\n",
      "epoch 0/1, iter 204/391, loss 1.460\n",
      "epoch 0/1, iter 205/391, loss 1.774\n",
      "epoch 0/1, iter 206/391, loss 1.709\n",
      "epoch 0/1, iter 207/391, loss 1.547\n",
      "epoch 0/1, iter 208/391, loss 1.596\n",
      "epoch 0/1, iter 209/391, loss 1.706\n",
      "epoch 0/1, iter 210/391, loss 1.564\n",
      "epoch 0/1, iter 211/391, loss 1.693\n",
      "epoch 0/1, iter 212/391, loss 1.561\n",
      "epoch 0/1, iter 213/391, loss 1.539\n",
      "epoch 0/1, iter 214/391, loss 1.658\n",
      "epoch 0/1, iter 215/391, loss 1.542\n",
      "epoch 0/1, iter 216/391, loss 1.692\n",
      "epoch 0/1, iter 217/391, loss 1.469\n",
      "epoch 0/1, iter 218/391, loss 1.664\n",
      "epoch 0/1, iter 219/391, loss 1.701\n",
      "epoch 0/1, iter 220/391, loss 1.600\n",
      "epoch 0/1, iter 221/391, loss 1.576\n",
      "epoch 0/1, iter 222/391, loss 1.606\n",
      "epoch 0/1, iter 223/391, loss 1.602\n",
      "epoch 0/1, iter 224/391, loss 1.799\n",
      "epoch 0/1, iter 225/391, loss 1.528\n",
      "epoch 0/1, iter 226/391, loss 1.616\n",
      "epoch 0/1, iter 227/391, loss 1.613\n",
      "epoch 0/1, iter 228/391, loss 1.653\n",
      "epoch 0/1, iter 229/391, loss 1.490\n",
      "epoch 0/1, iter 230/391, loss 1.573\n",
      "epoch 0/1, iter 231/391, loss 1.581\n",
      "epoch 0/1, iter 232/391, loss 1.545\n",
      "epoch 0/1, iter 233/391, loss 1.502\n",
      "epoch 0/1, iter 234/391, loss 1.418\n",
      "epoch 0/1, iter 235/391, loss 1.534\n",
      "epoch 0/1, iter 236/391, loss 1.481\n",
      "epoch 0/1, iter 237/391, loss 1.451\n",
      "epoch 0/1, iter 238/391, loss 1.574\n",
      "epoch 0/1, iter 239/391, loss 1.734\n",
      "epoch 0/1, iter 240/391, loss 1.522\n",
      "epoch 0/1, iter 241/391, loss 1.445\n",
      "epoch 0/1, iter 242/391, loss 1.552\n",
      "epoch 0/1, iter 243/391, loss 1.511\n",
      "epoch 0/1, iter 244/391, loss 1.505\n",
      "epoch 0/1, iter 245/391, loss 1.617\n",
      "epoch 0/1, iter 246/391, loss 1.645\n",
      "epoch 0/1, iter 247/391, loss 1.548\n",
      "epoch 0/1, iter 248/391, loss 1.508\n",
      "epoch 0/1, iter 249/391, loss 1.491\n",
      "epoch 0/1, iter 250/391, loss 1.513\n",
      "epoch 0/1, iter 251/391, loss 1.513\n",
      "epoch 0/1, iter 252/391, loss 1.560\n",
      "epoch 0/1, iter 253/391, loss 1.620\n",
      "epoch 0/1, iter 254/391, loss 1.545\n",
      "epoch 0/1, iter 255/391, loss 1.608\n",
      "epoch 0/1, iter 256/391, loss 1.670\n",
      "epoch 0/1, iter 257/391, loss 1.406\n",
      "epoch 0/1, iter 258/391, loss 1.578\n",
      "epoch 0/1, iter 259/391, loss 1.657\n",
      "epoch 0/1, iter 260/391, loss 1.453\n",
      "epoch 0/1, iter 261/391, loss 1.503\n",
      "epoch 0/1, iter 262/391, loss 1.487\n",
      "epoch 0/1, iter 263/391, loss 1.498\n",
      "epoch 0/1, iter 264/391, loss 1.549\n",
      "epoch 0/1, iter 265/391, loss 1.549\n",
      "epoch 0/1, iter 266/391, loss 1.622\n",
      "epoch 0/1, iter 267/391, loss 1.434\n",
      "epoch 0/1, iter 268/391, loss 1.621\n",
      "epoch 0/1, iter 269/391, loss 1.562\n",
      "epoch 0/1, iter 270/391, loss 1.445\n",
      "epoch 0/1, iter 271/391, loss 1.476\n",
      "epoch 0/1, iter 272/391, loss 1.387\n",
      "epoch 0/1, iter 273/391, loss 1.245\n",
      "epoch 0/1, iter 274/391, loss 1.624\n",
      "epoch 0/1, iter 275/391, loss 1.451\n",
      "epoch 0/1, iter 276/391, loss 1.410\n",
      "epoch 0/1, iter 277/391, loss 1.478\n",
      "epoch 0/1, iter 278/391, loss 1.440\n",
      "epoch 0/1, iter 279/391, loss 1.408\n",
      "epoch 0/1, iter 280/391, loss 1.496\n",
      "epoch 0/1, iter 281/391, loss 1.589\n",
      "epoch 0/1, iter 282/391, loss 1.500\n",
      "epoch 0/1, iter 283/391, loss 1.473\n",
      "epoch 0/1, iter 284/391, loss 1.573\n",
      "epoch 0/1, iter 285/391, loss 1.538\n",
      "epoch 0/1, iter 286/391, loss 1.531\n",
      "epoch 0/1, iter 287/391, loss 1.584\n",
      "epoch 0/1, iter 288/391, loss 1.391\n",
      "epoch 0/1, iter 289/391, loss 1.431\n",
      "epoch 0/1, iter 290/391, loss 1.500\n",
      "epoch 0/1, iter 291/391, loss 1.511\n",
      "epoch 0/1, iter 292/391, loss 1.417\n",
      "epoch 0/1, iter 293/391, loss 1.454\n",
      "epoch 0/1, iter 294/391, loss 1.459\n",
      "epoch 0/1, iter 295/391, loss 1.540\n",
      "epoch 0/1, iter 296/391, loss 1.475\n",
      "epoch 0/1, iter 297/391, loss 1.502\n",
      "epoch 0/1, iter 298/391, loss 1.524\n",
      "epoch 0/1, iter 299/391, loss 1.561\n",
      "epoch 0/1, iter 300/391, loss 1.522\n",
      "epoch 0/1, iter 301/391, loss 1.482\n",
      "epoch 0/1, iter 302/391, loss 1.457\n",
      "epoch 0/1, iter 303/391, loss 1.297\n",
      "epoch 0/1, iter 304/391, loss 1.390\n",
      "epoch 0/1, iter 305/391, loss 1.419\n",
      "epoch 0/1, iter 306/391, loss 1.302\n",
      "epoch 0/1, iter 307/391, loss 1.366\n",
      "epoch 0/1, iter 308/391, loss 1.357\n",
      "epoch 0/1, iter 309/391, loss 1.341\n",
      "epoch 0/1, iter 310/391, loss 1.444\n",
      "epoch 0/1, iter 311/391, loss 1.609\n",
      "epoch 0/1, iter 312/391, loss 1.617\n",
      "epoch 0/1, iter 313/391, loss 1.408\n",
      "epoch 0/1, iter 314/391, loss 1.446\n",
      "epoch 0/1, iter 315/391, loss 1.424\n",
      "epoch 0/1, iter 316/391, loss 1.464\n",
      "epoch 0/1, iter 317/391, loss 1.507\n",
      "epoch 0/1, iter 318/391, loss 1.345\n",
      "epoch 0/1, iter 319/391, loss 1.431\n",
      "epoch 0/1, iter 320/391, loss 1.608\n",
      "epoch 0/1, iter 321/391, loss 1.317\n",
      "epoch 0/1, iter 322/391, loss 1.419\n",
      "epoch 0/1, iter 323/391, loss 1.665\n",
      "epoch 0/1, iter 324/391, loss 1.374\n",
      "epoch 0/1, iter 325/391, loss 1.661\n",
      "epoch 0/1, iter 326/391, loss 1.487\n",
      "epoch 0/1, iter 327/391, loss 1.545\n",
      "epoch 0/1, iter 328/391, loss 1.448\n",
      "epoch 0/1, iter 329/391, loss 1.572\n",
      "epoch 0/1, iter 330/391, loss 1.466\n",
      "epoch 0/1, iter 331/391, loss 1.371\n",
      "epoch 0/1, iter 332/391, loss 1.522\n",
      "epoch 0/1, iter 333/391, loss 1.478\n",
      "epoch 0/1, iter 334/391, loss 1.565\n",
      "epoch 0/1, iter 335/391, loss 1.566\n",
      "epoch 0/1, iter 336/391, loss 1.433\n",
      "epoch 0/1, iter 337/391, loss 1.310\n",
      "epoch 0/1, iter 338/391, loss 1.362\n",
      "epoch 0/1, iter 339/391, loss 1.356\n",
      "epoch 0/1, iter 340/391, loss 1.237\n",
      "epoch 0/1, iter 341/391, loss 1.452\n",
      "epoch 0/1, iter 342/391, loss 1.456\n",
      "epoch 0/1, iter 343/391, loss 1.418\n",
      "epoch 0/1, iter 344/391, loss 1.237\n",
      "epoch 0/1, iter 345/391, loss 1.363\n",
      "epoch 0/1, iter 346/391, loss 1.640\n",
      "epoch 0/1, iter 347/391, loss 1.453\n",
      "epoch 0/1, iter 348/391, loss 1.323\n",
      "epoch 0/1, iter 349/391, loss 1.381\n",
      "epoch 0/1, iter 350/391, loss 1.433\n",
      "epoch 0/1, iter 351/391, loss 1.380\n",
      "epoch 0/1, iter 352/391, loss 1.365\n",
      "epoch 0/1, iter 353/391, loss 1.277\n",
      "epoch 0/1, iter 354/391, loss 1.347\n",
      "epoch 0/1, iter 355/391, loss 1.345\n",
      "epoch 0/1, iter 356/391, loss 1.413\n",
      "epoch 0/1, iter 357/391, loss 1.628\n",
      "epoch 0/1, iter 358/391, loss 1.322\n",
      "epoch 0/1, iter 359/391, loss 1.150\n",
      "epoch 0/1, iter 360/391, loss 1.496\n",
      "epoch 0/1, iter 361/391, loss 1.513\n",
      "epoch 0/1, iter 362/391, loss 1.374\n",
      "epoch 0/1, iter 363/391, loss 1.807\n",
      "epoch 0/1, iter 364/391, loss 1.503\n",
      "epoch 0/1, iter 365/391, loss 1.318\n",
      "epoch 0/1, iter 366/391, loss 1.389\n",
      "epoch 0/1, iter 367/391, loss 1.415\n",
      "epoch 0/1, iter 368/391, loss 1.284\n",
      "epoch 0/1, iter 369/391, loss 1.585\n",
      "epoch 0/1, iter 370/391, loss 1.225\n",
      "epoch 0/1, iter 371/391, loss 1.448\n",
      "epoch 0/1, iter 372/391, loss 1.412\n",
      "epoch 0/1, iter 373/391, loss 1.395\n",
      "epoch 0/1, iter 374/391, loss 1.292\n",
      "epoch 0/1, iter 375/391, loss 1.332\n",
      "epoch 0/1, iter 376/391, loss 1.360\n",
      "epoch 0/1, iter 377/391, loss 1.274\n",
      "epoch 0/1, iter 378/391, loss 1.307\n",
      "epoch 0/1, iter 379/391, loss 1.506\n",
      "epoch 0/1, iter 380/391, loss 1.277\n",
      "epoch 0/1, iter 381/391, loss 1.446\n",
      "epoch 0/1, iter 382/391, loss 1.404\n",
      "epoch 0/1, iter 383/391, loss 1.399\n",
      "epoch 0/1, iter 384/391, loss 1.361\n",
      "epoch 0/1, iter 385/391, loss 1.549\n",
      "epoch 0/1, iter 386/391, loss 1.434\n",
      "epoch 0/1, iter 387/391, loss 1.216\n",
      "epoch 0/1, iter 388/391, loss 1.384\n",
      "epoch 0/1, iter 389/391, loss 1.553\n",
      "epoch 0/1, iter 390/391, loss 1.288\n",
      "epoch 1, loss 1.7084, train acc 0.367, test acc 0.515, time 2734.7 sec\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_ch05(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}