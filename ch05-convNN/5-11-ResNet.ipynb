{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.11 ResNet残差网络\n",
    "\n",
    "关于ResNet：\n",
    "\n",
    "+ 实际中，添加过多的层后，训练误差反而会增加，\n",
    "\n",
    "+ 使用批量归一化，能使深层网络的中间数值变得稳定，但不能完全解决该问题，\n",
    "\n",
    "+ 2015年，何恺明提出的残差网络ResNet，较好的解决了深度网络的训练误差上升的问题。\n",
    "\n",
    "+ ResNet一经提出（2015年），就夺得了ImageNet图像识别挑战赛冠军。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import d2l_pytorch as d2l \n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义残差块\n",
    "# 残差块,它可以设定输出通道数、\n",
    "# 是否使用额外的1×1卷积层来修改通道数\n",
    "# 以及卷积层的步幅\n",
    "\n",
    "class residual(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,use_1conv=False,stride=1):\n",
    "        super(residual,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1)\n",
    "        self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1)\n",
    "        if use_1conv:\n",
    "            self.conv3=nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=stride)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=F.relu(self.bn1(self.conv1(x)))\n",
    "        y=self.bn2(self.conv2(y))\n",
    "        if self.conv3:\n",
    "            x=self.conv3(x)\n",
    "        return F.relu(y+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 6, 6])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# test\n",
    "blk = residual(3, 3)\n",
    "X = torch.rand((4, 3, 6, 6))\n",
    "blk(X).shape # torch.Size([4, 3, 6, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 3, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "blk = residual(3, 6, use_1conv=True, stride=2)\n",
    "blk(X).shape # torch.Size([4, 6, 3, 3])\n"
   ]
  },
  {
   "source": [
    "ResNet模型\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(\n",
    "    nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 后面接4个残差块组成的模块\n",
    "\n",
    "# 第一个模块的通道数与输入的通道数相同\n",
    "def resnet_blk(in_channels,out_channels,num_residual,first_blk=False):\n",
    "    if first_blk:\n",
    "        assert in_channels==out_channels\n",
    "    blk=[]\n",
    "    for i in range(num_residual):\n",
    "        if i==0 and not first_blk:\n",
    "            blk.append(residual(in_channels,out_channels,use_1conv=True,stride=2))\n",
    "        else:\n",
    "            blk.append(residual(out_channels,out_channels))\n",
    "    return nn.Sequential(*blk) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个模块，使用2个残差块\n",
    "\n",
    "net.add_module('resnet_block1',resnet_blk(64,64,2,first_blk=True))\n",
    "net.add_module('resnet_block2',resnet_blk(64,128,2))\n",
    "net.add_module('resnet_block3',resnet_blk(128,256,2))\n",
    "net.add_module('resnet_block4',resnet_blk(256,512,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入全局平均池化层，全连接层输出\n",
    "net.add_module('global_avg_pool',d2l.GlobalAvgPool2d())\n",
    "net.add_module('fc',nn.Sequential(d2l.FlattenLayer(),nn.Linear(512,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  output shape:\t torch.Size([1, 64, 112, 112])\n1  output shape:\t torch.Size([1, 64, 112, 112])\n2  output shape:\t torch.Size([1, 64, 112, 112])\n3  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block1  output shape:\t torch.Size([1, 64, 56, 56])\nresnet_block2  output shape:\t torch.Size([1, 128, 28, 28])\nresnet_block3  output shape:\t torch.Size([1, 256, 14, 14])\nresnet_block4  output shape:\t torch.Size([1, 512, 7, 7])\nglobal_avg_pool  output shape:\t torch.Size([1, 512, 1, 1])\nfc  output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "X = torch.rand((1, 1, 224, 224))\n",
    "for name, layer in net.named_children():\n",
    "    X = layer(X)\n",
    "    print(name, ' output shape:\\t', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据，训练模型\n",
    "\n",
    "batch_size=512\n",
    "\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist_ch05(batch_size,resize=96)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs=0.001,1  # 5\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training on  cpu\n",
      "epoch 0/1, iter 0/117, loss 2.353\n",
      "epoch 0/1, iter 1/117, loss 1.957\n",
      "epoch 0/1, iter 2/117, loss 2.378\n",
      "epoch 0/1, iter 3/117, loss 1.593\n",
      "epoch 0/1, iter 4/117, loss 1.145\n",
      "epoch 0/1, iter 5/117, loss 1.138\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "d2l.train_ch05(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "source": [
    "残差块通过跨层的数据通道从而能够训练出有效的深度神经网络."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}