{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# BN-Inception\n",
    "\n",
    "关于该网络：\n",
    "\n",
    "+ 基于GoogleNet, 加入了BN层,\n",
    "\n",
    "+ GoogLeNet的基础块Inception,结构如下：\n",
    "\n",
    "![img](https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.9_inception.svg)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time \n",
    "from torch import nn,optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from script import d2l_pytorch as d2l \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,**args):\n",
    "        super(baseConv2d,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels,out_channels,**args)\n",
    "        self.bn=nn.BatchNorm2d(out_channels)\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # 4条线路\n",
    "    def __init__(self,in_c,c1,c2,c3,c4,batch_norm=False):\n",
    "        super(Inception,self).__init__()\n",
    "        if batch_norm==False:\n",
    "            # 线路1，共1层，就是最左侧的那条\n",
    "            ## 1*1的卷积层，用来减少通道数\n",
    "            self.p1_1=nn.Conv2d(in_channels=in_c,out_channels=c1,kernel_size=1)\n",
    "            # 线路2，共2层\n",
    "            ## 1*1的卷积层\n",
    "            self.p2_1=nn.Conv2d(in_channels=in_c,out_channels=c2[0],kernel_size=1)\n",
    "            ## 3*3的卷积层\n",
    "            self.p2_2=nn.Conv2d(c2[0],c2[1],kernel_size=3,padding=1)\n",
    "            # 线路3，共2层\n",
    "            ## 1*1的卷积层\n",
    "            self.p3_1=nn.Conv2d(in_c,c3[0],kernel_size=1)\n",
    "            ## 5*5的卷积层\n",
    "            self.p3_2=nn.Conv2d(c3[0],c3[1],kernel_size=5,padding=2)\n",
    "            # 线路4，共2层\n",
    "            ## 3*3的最大池化层\n",
    "            self.p4_1=nn.MaxPool2d(kernel_size=3,stride=1,padding=1)\n",
    "            self.p4_2=nn.Conv2d(in_c,c4,kernel_size=1)\n",
    "        else:\n",
    "            self.p1_1=baseConv2d(in_c,c1,kernel_size=1)\n",
    "            # 线路2，共2层\n",
    "            self.p2_1=baseConv2d(in_c,c2[0],kernel_size=1)\n",
    "            self.p2_2=baseConv2d(c2[0],c2[1],kernel_size=3,padding=1)\n",
    "            \n",
    "            # 线路3，共2层\n",
    "            self.p3_1=baseConv2d(in_c,c3[0],kernel_size=1)\n",
    "            self.p3_2=baseConv2d(c3[0],c3[1],kernel_size=5,padding=2)\n",
    "            \n",
    "            # 线路4，共2层\n",
    "            self.p4_1=baseConv2d(kernel_size=3,stride=1,padding=1)\n",
    "            self.p4_2=baseConv2d(in_c,c4,kernel_size=1)\n",
    "\n",
    "        # relu\n",
    "    def forward(self,x):\n",
    "        # 线路1\n",
    "        p1=F.relu(self.p1_1(x))\n",
    "        # 线路2\n",
    "        p2=F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        # 线路3\n",
    "        p3=F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        # 线路4\n",
    "        p4=F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 将四条线路的输出，在通道维上连结\n",
    "        # out=[p1,p2,p3,p4];torch.cat(out,1)\n",
    "        return torch.cat((p1,p2,p3,p4),dim=1)  "
   ]
  },
  {
   "source": [
    "+ GoogLeNet模型，在主体部分使用5个模块。\n",
    "\n",
    "    每个模块之间使用步幅为2的3\\*3池化层来减小输出的高和宽，\n",
    "\n",
    "    每一个模块使用通道数为7\\*7的卷积层。\n",
    "\n",
    "模块1和模块2如图：\n",
    "\n",
    "<img src=\"https://gitee.com/changyv/md-pic/raw/master/20210317211018.png\" style=\"zoom:50%;\" />"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=nn.Sequential(\n",
    "    # 3-->1\n",
    "    nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")\n",
    "b2=nn.Sequential(\n",
    "    # 1*1的卷积层\n",
    "    nn.Conv2d(64,64,kernel_size=1),\n",
    "    # 3*3的卷积层，将通道数增加3倍\n",
    "    nn.Conv2d(64,192,kernel_size=3,padding=1),\n",
    "    # 池化层\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第3个模块，串联2个完整的Inception块，\n",
    "\n",
    "第一个模块输出通道数为：64+128+32+32=256\n",
    "\n",
    "第二个模块输出通道数为：128+192+96+64=480\n",
    "\n",
    "如图：\n",
    "\n",
    "<img src=\"https://gitee.com/changyv/md-pic/raw/master/20210317212459.png\" alt=\"image-20210317212456643\" style=\"zoom:50%;\" />\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3=nn.Sequential(\n",
    "    Inception(192,64,(96,128),(16,32),32),\n",
    "    Inception(256,128,(128,192),(32,96),64),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第四个模块，串联了5个Inception块，输出通道数分别为：\n",
    "\n",
    "+ 192+208+48+64=512\n",
    "\n",
    "+ 160+224+64+64=512\n",
    "\n",
    "+ 128+256+64+64=512、\n",
    "\n",
    "+ 112+288+64+64=528\n",
    "\n",
    "+ 256+320+128+128=832"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(\n",
    "    Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "    Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "    Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "    Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "    Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ]
  },
  {
   "source": [
    "第5个模块，串联了2个Inception模块，其输出通道分别为：\n",
    "\n",
    "+ 256+320+128+128=832\n",
    "\n",
    "+ 384+384+128+128=1024\n",
    "\n",
    "第5个模块后面，跟着输出层，输出层使用全局平均池化层，将每个通道的高和宽变为1，最后将输出变为二维数组，后接一个输出个数为类别的全连接层。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(\n",
    "    Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "    Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "    d2l.GlobalAvgPool2d()\n",
    ")"
   ]
  },
  {
   "source": [
    "输出层"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    b1, b2, b3, b4, b5, \n",
    "    d2l.FlattenLayer(), \n",
    "    nn.Linear(1024, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (1): Sequential(\n    (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n    (1): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (2): Sequential(\n    (0): Inception(\n      (p1_1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (1): Inception(\n      (p1_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (3): Sequential(\n    (0): Inception(\n      (p1_1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (1): Inception(\n      (p1_1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (2): Inception(\n      (p1_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (3): Inception(\n      (p1_1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (4): Inception(\n      (p1_1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  )\n  (4): Sequential(\n    (0): Inception(\n      (p1_1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (1): Inception(\n      (p1_1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n      (p2_1): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n      (p2_2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (p3_1): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n      (p3_2): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n      (p4_1): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n      (p4_2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n    )\n    (2): GlobalAvgPool2d()\n  )\n  (5): FlattenLayer()\n  (6): Linear(in_features=1024, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "out shape: torch.Size([1, 64, 24, 24])\nout shape: torch.Size([1, 192, 12, 12])\nout shape: torch.Size([1, 480, 6, 6])\nout shape: torch.Size([1, 832, 3, 3])\nout shape: torch.Size([1, 1024, 1, 1])\nout shape: torch.Size([1, 1024])\nout shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "# 将输入的高和宽从224降到96，简化计算\n",
    "\n",
    "# X=torch.rand(1,3,96,96)\n",
    "X=torch.rand(1,1,96,96)\n",
    "\n",
    "for blk in net.children():\n",
    "    X=blk(X)\n",
    "    print('out shape:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据，训练模型\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist_resize(batch_size,resize=96)\n",
    "\n",
    "lr,num_epochs=0.001,2\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=iter(train_iter).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 96])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "x[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training on  cpu\n",
      "epoch 0/2, iter 0/937, loss 2.303\n",
      "epoch 0/2, iter 1/937, loss 2.309\n",
      "epoch 0/2, iter 2/937, loss 2.302\n",
      "epoch 0/2, iter 3/937, loss 2.310\n",
      "epoch 0/2, iter 4/937, loss 2.320\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "d2l.train(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "source": [
    "## 小结\n",
    "\n",
    "+ Inception有4个并行的子网络，使用不同窗口的卷积层和最大池化层来并行的抽取信息，使用1\\*1的卷积层减少通道数，\n",
    "\n",
    "+ GoogLeNet将都哦个Inception块与其它块串联起来，每个Inception块的通道分配数的比值，实在ImageNet上大量实验获得，\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}