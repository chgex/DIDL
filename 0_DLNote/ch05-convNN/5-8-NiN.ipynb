{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NiN，网络中的网络\n",
    "\n",
    "AlexNet和VGG在设计上的共同之处：\n",
    "\n",
    "+ 首先使用卷积层构成的模块，充分抽取空间特征，\n",
    "\n",
    "+ 然后使用全连接层构成的模块，输出分类结果。\n",
    "\n",
    "> 相比于LeNet，AlexNet和VGG改进主要在于模块变得更宽(增加通道数)，网络变得更深。\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "+ 卷积层的输入和输出是四维数组：(样本，通道，高，宽)， \n",
    "\n",
    "+ 全连接层的输入和输出是二维数组：(样本，特征)， \n",
    "\n",
    "+ 如果在全连接层后面接一个卷积层，则需要变换维度，\n",
    "\n",
    "+ 1\\*1的卷积层，等价于全连接层：高和宽上的每个元素，相当于一个样本，通道相当于特征。\n",
    "\n",
    "NiN在卷积层之后，使用1\\*1的卷积层作为全连接层。卷积层和1\\*1的卷积层，共同构成一个块。NIN网络就是由这样的块所组成的。\n",
    "\n",
    "NiN块，代码："
   ]
  },
  {
   "source": [
    "import time \n",
    "import torch\n",
    "from torch import nn,optim \n",
    "\n",
    "import d2l_pytorch as d2l \n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nin_block(in_channels,out_channels,kernel_size,stride,padding):\n",
    "    blk=nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels,out_channels,kernel_size=1),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "    return blk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NiN模型\n",
    "\n",
    "NiN使用的卷积窗口分别为：11\\*11,5\\*5,3\\*3，输出通道与AlexNet一致。\n",
    "\n",
    "每个blk后面接一个步幅为2，窗口大小为3\\*3的池化层。\n",
    "\n",
    "相比于AlexNet，NiN去掉了最后的3个全连接层，取而代之的是：输出通道为标签个数的NiN块。最后，网络使用全局平均池化层，对每个通道中所有元素求平均，以直接用来分类。\n",
    "> 全局平均池化层，就是窗口形状等于输入的平均池化层。\n",
    "\n",
    "输出通道为标签个数的NiN块代替全脸基层的好处是：能防止过拟合。\n",
    "> 全连接输出层，容易造成过拟合。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义全局池化层\n",
    "\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d,self).__init__()\n",
    "    def forward(self,x):\n",
    "        # 全局池化层的大小，等于输入的高和宽\n",
    "        return F.avg_pool2d(x,kernel_size=x.size()[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NiN():\n",
    "    net=nn.Sequential(\n",
    "        nin_block(1,96,kernel_size=11,stride=4,padding=0),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "        nin_block(96,256,kernel_size=5,stride=1,padding=2),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "        nin_block(256,384,kernel_size=3,stride=1,padding=1),\n",
    "        nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "        nn.Dropout(0.5),\n",
    "        # 10类\n",
    "        nin_block(384,10,kernel_size=3,stride=1,padding=1),\n",
    "        GlobalAvgPool2d(),\n",
    "        # 将4维输出，转为2维输出，\n",
    "        # 其大小为(批量，10)\n",
    "        d2l.FlattenLayer()\n",
    "    )\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 output shape: torch.Size([1, 96, 54, 54])\n",
      "1 output shape: torch.Size([1, 96, 26, 26])\n",
      "2 output shape: torch.Size([1, 256, 26, 26])\n",
      "3 output shape: torch.Size([1, 256, 12, 12])\n",
      "4 output shape: torch.Size([1, 384, 12, 12])\n",
      "5 output shape: torch.Size([1, 384, 5, 5])\n",
      "6 output shape: torch.Size([1, 384, 5, 5])\n",
      "7 output shape: torch.Size([1, 10, 5, 5])\n",
      "8 output shape: torch.Size([1, 10, 1, 1])\n",
      "9 output shape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "net=NiN()\n",
    "X=torch.rand(1,1,224,224)\n",
    "for name,blk in net.named_children():\n",
    "    X=blk(X)\n",
    "    print(name,'output shape:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取数据\n",
    "batch_size=32\n",
    "# 60000,10000\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist_ch05(batch_size,resize=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "lr,num_epochs=0.002,5\n",
    "\n",
    "optimizer=torch.optim.Adam(net.parameters(),lr=lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 0/5, iter 0/1875, loss 2.308\n",
      "epoch 0/5, iter 1/1875, loss 2.340\n",
      "epoch 0/5, iter 2/1875, loss 2.300\n",
      "epoch 0/5, iter 3/1875, loss 2.322\n",
      "epoch 0/5, iter 4/1875, loss 2.327\n",
      "epoch 0/5, iter 5/1875, loss 2.310\n",
      "epoch 0/5, iter 6/1875, loss 2.278\n",
      "epoch 0/5, iter 7/1875, loss 2.293\n",
      "epoch 0/5, iter 8/1875, loss 2.309\n",
      "epoch 0/5, iter 9/1875, loss 2.321\n",
      "epoch 0/5, iter 10/1875, loss 2.308\n",
      "epoch 0/5, iter 11/1875, loss 2.289\n",
      "epoch 0/5, iter 12/1875, loss 2.302\n",
      "epoch 0/5, iter 13/1875, loss 2.337\n",
      "epoch 0/5, iter 14/1875, loss 2.325\n",
      "epoch 0/5, iter 15/1875, loss 2.311\n",
      "epoch 0/5, iter 16/1875, loss 2.329\n",
      "epoch 0/5, iter 17/1875, loss 2.317\n",
      "epoch 0/5, iter 18/1875, loss 2.284\n",
      "epoch 0/5, iter 19/1875, loss 2.324\n",
      "epoch 0/5, iter 20/1875, loss 2.305\n",
      "epoch 0/5, iter 21/1875, loss 2.303\n",
      "epoch 0/5, iter 22/1875, loss 2.291\n",
      "epoch 0/5, iter 23/1875, loss 2.333\n",
      "epoch 0/5, iter 24/1875, loss 2.335\n",
      "epoch 0/5, iter 25/1875, loss 2.327\n",
      "epoch 0/5, iter 26/1875, loss 2.290\n",
      "epoch 0/5, iter 27/1875, loss 2.313\n",
      "epoch 0/5, iter 28/1875, loss 2.324\n",
      "epoch 0/5, iter 29/1875, loss 2.331\n",
      "epoch 0/5, iter 30/1875, loss 2.309\n",
      "epoch 0/5, iter 31/1875, loss 2.277\n",
      "epoch 0/5, iter 32/1875, loss 2.286\n",
      "epoch 0/5, iter 33/1875, loss 2.320\n",
      "epoch 0/5, iter 34/1875, loss 2.309\n",
      "epoch 0/5, iter 35/1875, loss 2.299\n",
      "epoch 0/5, iter 36/1875, loss 2.304\n",
      "epoch 0/5, iter 37/1875, loss 2.298\n",
      "epoch 0/5, iter 38/1875, loss 2.295\n",
      "epoch 0/5, iter 39/1875, loss 2.295\n",
      "epoch 0/5, iter 40/1875, loss 2.298\n",
      "epoch 0/5, iter 41/1875, loss 2.322\n",
      "epoch 0/5, iter 42/1875, loss 2.307\n",
      "epoch 0/5, iter 43/1875, loss 2.319\n",
      "epoch 0/5, iter 44/1875, loss 2.314\n",
      "epoch 0/5, iter 45/1875, loss 2.280\n",
      "epoch 0/5, iter 46/1875, loss 2.321\n",
      "epoch 0/5, iter 47/1875, loss 2.305\n",
      "epoch 0/5, iter 48/1875, loss 2.296\n",
      "epoch 0/5, iter 49/1875, loss 2.287\n",
      "epoch 0/5, iter 50/1875, loss 2.291\n",
      "epoch 0/5, iter 51/1875, loss 2.318\n",
      "epoch 0/5, iter 52/1875, loss 2.327\n",
      "epoch 0/5, iter 53/1875, loss 2.308\n",
      "epoch 0/5, iter 54/1875, loss 2.292\n",
      "epoch 0/5, iter 55/1875, loss 2.294\n",
      "epoch 0/5, iter 56/1875, loss 2.312\n",
      "epoch 0/5, iter 57/1875, loss 2.295\n",
      "epoch 0/5, iter 58/1875, loss 2.319\n",
      "epoch 0/5, iter 59/1875, loss 2.307\n",
      "epoch 0/5, iter 60/1875, loss 2.315\n",
      "epoch 0/5, iter 61/1875, loss 2.305\n",
      "epoch 0/5, iter 62/1875, loss 2.314\n",
      "epoch 0/5, iter 63/1875, loss 2.297\n",
      "epoch 0/5, iter 64/1875, loss 2.304\n",
      "epoch 0/5, iter 65/1875, loss 2.319\n",
      "epoch 0/5, iter 66/1875, loss 2.298\n",
      "epoch 0/5, iter 67/1875, loss 2.297\n",
      "epoch 0/5, iter 68/1875, loss 2.296\n",
      "epoch 0/5, iter 69/1875, loss 2.312\n",
      "epoch 0/5, iter 70/1875, loss 2.315\n",
      "epoch 0/5, iter 71/1875, loss 2.283\n",
      "epoch 0/5, iter 72/1875, loss 2.298\n",
      "epoch 0/5, iter 73/1875, loss 2.305\n",
      "epoch 0/5, iter 74/1875, loss 2.296\n",
      "epoch 0/5, iter 75/1875, loss 2.282\n",
      "epoch 0/5, iter 76/1875, loss 2.316\n",
      "epoch 0/5, iter 77/1875, loss 2.297\n",
      "epoch 0/5, iter 78/1875, loss 2.323\n",
      "epoch 0/5, iter 79/1875, loss 2.315\n",
      "epoch 0/5, iter 80/1875, loss 2.301\n",
      "epoch 0/5, iter 81/1875, loss 2.309\n",
      "epoch 0/5, iter 82/1875, loss 2.323\n",
      "epoch 0/5, iter 83/1875, loss 2.328\n",
      "epoch 0/5, iter 84/1875, loss 2.314\n",
      "epoch 0/5, iter 85/1875, loss 2.302\n",
      "epoch 0/5, iter 86/1875, loss 2.302\n",
      "epoch 0/5, iter 87/1875, loss 2.319\n",
      "epoch 0/5, iter 88/1875, loss 2.294\n",
      "epoch 0/5, iter 89/1875, loss 2.328\n",
      "epoch 0/5, iter 90/1875, loss 2.320\n",
      "epoch 0/5, iter 91/1875, loss 2.303\n",
      "epoch 0/5, iter 92/1875, loss 2.314\n",
      "epoch 0/5, iter 93/1875, loss 2.310\n",
      "epoch 0/5, iter 94/1875, loss 2.309\n",
      "epoch 0/5, iter 95/1875, loss 2.313\n",
      "epoch 0/5, iter 96/1875, loss 2.306\n",
      "epoch 0/5, iter 97/1875, loss 2.292\n",
      "epoch 0/5, iter 98/1875, loss 2.305\n",
      "epoch 0/5, iter 99/1875, loss 2.305\n",
      "epoch 0/5, iter 100/1875, loss 2.306\n",
      "epoch 0/5, iter 101/1875, loss 2.290\n",
      "epoch 0/5, iter 102/1875, loss 2.304\n",
      "epoch 0/5, iter 103/1875, loss 2.305\n",
      "epoch 0/5, iter 104/1875, loss 2.304\n",
      "epoch 0/5, iter 105/1875, loss 2.303\n",
      "epoch 0/5, iter 106/1875, loss 2.291\n",
      "epoch 0/5, iter 107/1875, loss 2.274\n",
      "epoch 0/5, iter 108/1875, loss 2.286\n",
      "epoch 0/5, iter 109/1875, loss 2.272\n",
      "epoch 0/5, iter 110/1875, loss 2.259\n",
      "epoch 0/5, iter 111/1875, loss 2.241\n",
      "epoch 0/5, iter 112/1875, loss 2.396\n",
      "epoch 0/5, iter 113/1875, loss 2.243\n",
      "epoch 0/5, iter 114/1875, loss 2.276\n",
      "epoch 0/5, iter 115/1875, loss 2.290\n",
      "epoch 0/5, iter 116/1875, loss 2.297\n",
      "epoch 0/5, iter 117/1875, loss 2.283\n",
      "epoch 0/5, iter 118/1875, loss 2.301\n",
      "epoch 0/5, iter 119/1875, loss 2.308\n",
      "epoch 0/5, iter 120/1875, loss 2.316\n",
      "epoch 0/5, iter 121/1875, loss 2.292\n",
      "epoch 0/5, iter 122/1875, loss 2.259\n",
      "epoch 0/5, iter 123/1875, loss 2.285\n",
      "epoch 0/5, iter 124/1875, loss 2.280\n",
      "epoch 0/5, iter 125/1875, loss 2.214\n",
      "epoch 0/5, iter 126/1875, loss 2.254\n",
      "epoch 0/5, iter 127/1875, loss 2.232\n",
      "epoch 0/5, iter 128/1875, loss 2.235\n",
      "epoch 0/5, iter 129/1875, loss 2.342\n",
      "epoch 0/5, iter 130/1875, loss 2.183\n",
      "epoch 0/5, iter 131/1875, loss 2.189\n",
      "epoch 0/5, iter 132/1875, loss 2.295\n",
      "epoch 0/5, iter 133/1875, loss 2.238\n",
      "epoch 0/5, iter 134/1875, loss 2.230\n",
      "epoch 0/5, iter 135/1875, loss 2.149\n",
      "epoch 0/5, iter 136/1875, loss 2.148\n",
      "epoch 0/5, iter 137/1875, loss 2.223\n",
      "epoch 0/5, iter 138/1875, loss 1.939\n",
      "epoch 0/5, iter 139/1875, loss 2.213\n",
      "epoch 0/5, iter 140/1875, loss 2.217\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "d2l.train_ch05(net,train_iter,test_iter,batch_size,optimizer,device,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}