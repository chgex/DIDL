{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5.7使用重读元素的网络\n",
    "\n",
    "深度网络的设计思路：\n",
    "\n",
    "+ AlexNet\n",
    "\n",
    "+ VGG,重复使用简单的基础块来构建网络，\n",
    "\n",
    "...\n",
    "\n",
    "VGG块的组成：\n",
    "\n",
    "+ 填充为1，窗口为3\\*3的卷积层，\n",
    "\n",
    "+ 步幅为2，窗口为2\\*2的池化层，\n",
    "\n",
    "+ 卷积层之后，使用ReLU激活函数。\n",
    "\n",
    "对于给定的感受野（与输出有关的输入图片的局部大小），采用堆积的小卷积核优于采用大的卷积核，因为可以增加网络深度来保证学习更复杂的模式，而且代价还比较小（参数更少）。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch \n",
    "from torch import nn,optim\n",
    "import d2l_pytorch as d2l \n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始定义VGG网络的基础块\n",
    "# 该基础块包含：多个卷积层，一个最大池化层。\n",
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    blk=[]\n",
    "    for i in range(num_convs):\n",
    "        if i==0:\n",
    "            blk.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))\n",
    "        else:\n",
    "            blk.append(nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1))\n",
    "        blk.append(nn.ReLU())\n",
    "    blk.append(nn.MaxPool2d(kernel_size=2,stride=2))\n",
    "    return nn.Sequential(*blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义VGG网络的全连接层\n",
    "\n",
    "def vgg_fc(fc_features,fc_hiddens):\n",
    "    fc=nn.Sequential(d2l.FlattenLayer(),\n",
    "        nn.Linear(fc_features,fc_hiddens),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(fc_hiddens,fc_hiddens),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(fc_hiddens,10)\n",
    "    )\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始定义Vgg网络\n",
    "# 包括：5个卷积块，前2个使用单卷积层，后面3个使用双层卷积层。\n",
    "# VGG使用了8个卷积层和3个全连接层，所i又叫做VGG-11\n",
    "\n",
    "# vgg各个卷积块的超参数\n",
    "conv_arch=(\n",
    "    (1,  1, 64),\n",
    "    (1, 64,128),\n",
    "    (2,128,256),\n",
    "    (2,256,512),\n",
    "    (2,512,512)\n",
    ")\n",
    "# 使用的数据集是fashion-mnist，所以输入通道数为1.\n",
    "\n",
    "# VGG全连接层的参数\n",
    "fc_features=512*7*7     # c*w*h\n",
    "\n",
    "# VGG隐藏层的参数\n",
    "fc_hiddens=4096    # 任何？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始实现VGG网络\n",
    "\n",
    "def VGG(conv_arch,fc_features,fc_hiddens=4096):\n",
    "    net=nn.Sequential()\n",
    "    # 卷积层部分\n",
    "    for i,(num_convs,in_channels,out_channels) in enumerate(conv_arch):\n",
    "        net.add_module('vgg_block_'+str(i+1),vgg_block(num_convs,in_channels,out_channels))\n",
    "    # 全连接层部分\n",
    "    net.add_module('fc',vgg_fc(fc_features,fc_hiddens))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vgg_block_1 output shape:  torch.Size([1, 64, 112, 112])\nvgg_block_2 output shape:  torch.Size([1, 128, 56, 56])\nvgg_block_3 output shape:  torch.Size([1, 256, 28, 28])\nvgg_block_4 output shape:  torch.Size([1, 512, 14, 14])\nvgg_block_5 output shape:  torch.Size([1, 512, 7, 7])\nfc output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "net = VGG(conv_arch, fc_features, fc_hiddens)\n",
    "\n",
    "X = torch.rand(1, 1, 224, 224)\n",
    "\n",
    "# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)\n",
    "for name, blk in net.named_children(): \n",
    "    X = blk(X)\n",
    "    print(name, 'output shape: ', X.shape)\n"
   ]
  },
  {
   "source": [
    "可以看到，每次我们将输入的高和宽减半，直到最终高和宽变成7后传入全连接层。与此同时，输出通道数每次翻倍，直到变成512。因为每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽、输入通道数和输出通道数的乘积成正比。VGG这种高和宽减半以及通道翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (vgg_block_1): Sequential(\n    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (vgg_block_2): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (vgg_block_3): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (vgg_block_4): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (vgg_block_5): Sequential(\n    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): FlattenLayer()\n    (1): Linear(in_features=3136, out_features=512, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=512, out_features=512, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.5, inplace=False)\n    (7): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# 获取数据，训练模型\n",
    "\n",
    "# 够高一个通道数更小的网络，在fashion_mnist数据集上进行训练\n",
    "\n",
    "ratio = 8\n",
    "small_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), \n",
    "                   (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\n",
    "net = VGG(small_conv_arch, fc_features // ratio, fc_hiddens // ratio)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "batch_size = 64 # 将数据分为64个批量，每一次完整的epoch，训练一个批量的数据。\n",
    "# 如出现“out of memory”的报错信息，可减小batch_size或resize\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist_ch05(batch_size, resize=224)"
   ]
  },
  {
   "source": [
    "# 开始训练\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "d2l.train_ch05_6(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "er 372/64, loss 0.475\n",
      "epoch 0/5, iter 373/64, loss 0.568\n",
      "epoch 0/5, iter 374/64, loss 0.604\n",
      "epoch 0/5, iter 375/64, loss 0.483\n",
      "epoch 0/5, iter 376/64, loss 0.303\n",
      "epoch 0/5, iter 377/64, loss 0.572\n",
      "epoch 0/5, iter 378/64, loss 0.393\n",
      "epoch 0/5, iter 379/64, loss 0.409\n",
      "epoch 0/5, iter 380/64, loss 0.570\n",
      "epoch 0/5, iter 381/64, loss 0.549\n",
      "epoch 0/5, iter 382/64, loss 0.517\n",
      "epoch 0/5, iter 383/64, loss 0.465\n",
      "epoch 0/5, iter 384/64, loss 0.521\n",
      "epoch 0/5, iter 385/64, loss 0.628\n",
      "epoch 0/5, iter 386/64, loss 0.517\n",
      "epoch 0/5, iter 387/64, loss 0.524\n",
      "epoch 0/5, iter 388/64, loss 0.578\n",
      "epoch 0/5, iter 389/64, loss 0.373\n",
      "epoch 0/5, iter 390/64, loss 0.447\n",
      "epoch 0/5, iter 391/64, loss 0.406\n",
      "epoch 0/5, iter 392/64, loss 0.526\n",
      "epoch 0/5, iter 393/64, loss 0.421\n",
      "epoch 0/5, iter 394/64, loss 0.476\n",
      "epoch 0/5, iter 395/64, loss 0.527\n",
      "epoch 0/5, iter 396/64, loss 0.539\n",
      "epoch 0/5, iter 397/64, loss 0.266\n",
      "epoch 0/5, iter 398/64, loss 0.470\n",
      "epoch 0/5, iter 399/64, loss 0.360\n",
      "epoch 0/5, iter 400/64, loss 0.469\n",
      "epoch 0/5, iter 401/64, loss 0.285\n",
      "epoch 0/5, iter 402/64, loss 0.464\n",
      "epoch 0/5, iter 403/64, loss 0.418\n",
      "epoch 0/5, iter 404/64, loss 0.449\n",
      "epoch 0/5, iter 405/64, loss 0.463\n",
      "epoch 0/5, iter 406/64, loss 0.407\n",
      "epoch 0/5, iter 407/64, loss 0.310\n",
      "epoch 0/5, iter 408/64, loss 0.479\n",
      "epoch 0/5, iter 409/64, loss 0.290\n",
      "epoch 0/5, iter 410/64, loss 0.366\n",
      "epoch 0/5, iter 411/64, loss 0.573\n",
      "epoch 0/5, iter 412/64, loss 0.303\n",
      "epoch 0/5, iter 413/64, loss 0.424\n",
      "epoch 0/5, iter 414/64, loss 0.744\n",
      "epoch 0/5, iter 415/64, loss 0.497\n",
      "epoch 0/5, iter 416/64, loss 0.398\n",
      "epoch 0/5, iter 417/64, loss 0.438\n",
      "epoch 0/5, iter 418/64, loss 0.502\n",
      "epoch 0/5, iter 419/64, loss 0.391\n",
      "epoch 0/5, iter 420/64, loss 0.389\n",
      "epoch 0/5, iter 421/64, loss 0.460\n",
      "epoch 0/5, iter 422/64, loss 0.529\n",
      "epoch 0/5, iter 423/64, loss 0.435\n",
      "epoch 0/5, iter 424/64, loss 0.485\n",
      "epoch 0/5, iter 425/64, loss 0.498\n",
      "epoch 0/5, iter 426/64, loss 0.357\n",
      "epoch 0/5, iter 427/64, loss 0.590\n",
      "epoch 0/5, iter 428/64, loss 0.433\n",
      "epoch 0/5, iter 429/64, loss 0.459\n",
      "epoch 0/5, iter 430/64, loss 0.598\n",
      "epoch 0/5, iter 431/64, loss 0.433\n",
      "epoch 0/5, iter 432/64, loss 0.418\n",
      "epoch 0/5, iter 433/64, loss 0.539\n",
      "epoch 0/5, iter 434/64, loss 0.479\n",
      "epoch 0/5, iter 435/64, loss 0.243\n",
      "epoch 0/5, iter 436/64, loss 0.426\n",
      "epoch 0/5, iter 437/64, loss 0.633\n",
      "epoch 0/5, iter 438/64, loss 0.339\n",
      "epoch 0/5, iter 439/64, loss 0.282\n",
      "epoch 0/5, iter 440/64, loss 0.473\n",
      "epoch 0/5, iter 441/64, loss 0.486\n",
      "epoch 0/5, iter 442/64, loss 0.327\n",
      "epoch 0/5, iter 443/64, loss 0.477\n",
      "epoch 0/5, iter 444/64, loss 0.404\n",
      "epoch 0/5, iter 445/64, loss 0.650\n",
      "epoch 0/5, iter 446/64, loss 0.360\n",
      "epoch 0/5, iter 447/64, loss 0.404\n",
      "epoch 0/5, iter 448/64, loss 0.525\n",
      "epoch 0/5, iter 449/64, loss 0.562\n",
      "epoch 0/5, iter 450/64, loss 0.378\n",
      "epoch 0/5, iter 451/64, loss 0.346\n",
      "epoch 0/5, iter 452/64, loss 0.318\n",
      "epoch 0/5, iter 453/64, loss 0.370\n",
      "epoch 0/5, iter 454/64, loss 0.297\n",
      "epoch 0/5, iter 455/64, loss 0.319\n",
      "epoch 0/5, iter 456/64, loss 0.462\n",
      "epoch 0/5, iter 457/64, loss 0.323\n",
      "epoch 0/5, iter 458/64, loss 0.674\n",
      "epoch 0/5, iter 459/64, loss 0.525\n",
      "epoch 0/5, iter 460/64, loss 0.319\n",
      "epoch 0/5, iter 461/64, loss 0.420\n",
      "epoch 0/5, iter 462/64, loss 0.372\n",
      "epoch 0/5, iter 463/64, loss 0.536\n",
      "epoch 0/5, iter 464/64, loss 0.539\n",
      "epoch 0/5, iter 465/64, loss 0.423\n",
      "epoch 0/5, iter 466/64, loss 0.514\n",
      "epoch 0/5, iter 467/64, loss 0.451\n",
      "epoch 0/5, iter 468/64, loss 0.546\n",
      "epoch 0/5, iter 469/64, loss 0.431\n",
      "epoch 0/5, iter 470/64, loss 0.527\n",
      "epoch 0/5, iter 471/64, loss 0.210\n",
      "epoch 0/5, iter 472/64, loss 0.383\n",
      "epoch 0/5, iter 473/64, loss 0.542\n",
      "epoch 0/5, iter 474/64, loss 0.672\n",
      "epoch 0/5, iter 475/64, loss 0.436\n",
      "epoch 0/5, iter 476/64, loss 0.375\n",
      "epoch 0/5, iter 477/64, loss 0.488\n",
      "epoch 0/5, iter 478/64, loss 0.431\n",
      "epoch 0/5, iter 479/64, loss 0.343\n",
      "epoch 0/5, iter 480/64, loss 0.366\n",
      "epoch 0/5, iter 481/64, loss 0.647\n",
      "epoch 0/5, iter 482/64, loss 0.491\n",
      "epoch 0/5, iter 483/64, loss 0.565\n",
      "epoch 0/5, iter 484/64, loss 0.644\n",
      "epoch 0/5, iter 485/64, loss 0.310\n",
      "epoch 0/5, iter 486/64, loss 0.342\n",
      "epoch 0/5, iter 487/64, loss 0.481\n",
      "epoch 0/5, iter 488/64, loss 0.447\n",
      "epoch 0/5, iter 489/64, loss 0.323\n",
      "epoch 0/5, iter 490/64, loss 0.227\n",
      "epoch 0/5, iter 491/64, loss 0.493\n",
      "epoch 0/5, iter 492/64, loss 0.602\n",
      "epoch 0/5, iter 493/64, loss 0.374\n",
      "epoch 0/5, iter 494/64, loss 0.510\n",
      "epoch 0/5, iter 495/64, loss 0.459\n",
      "epoch 0/5, iter 496/64, loss 0.566\n",
      "epoch 0/5, iter 497/64, loss 0.570\n",
      "epoch 0/5, iter 498/64, loss 0.551\n",
      "epoch 0/5, iter 499/64, loss 0.285\n",
      "epoch 0/5, iter 500/64, loss 0.432\n",
      "epoch 0/5, iter 501/64, loss 0.287\n",
      "epoch 0/5, iter 502/64, loss 0.387\n",
      "epoch 0/5, iter 503/64, loss 0.486\n",
      "epoch 0/5, iter 504/64, loss 0.405\n",
      "epoch 0/5, iter 505/64, loss 0.233\n",
      "epoch 0/5, iter 506/64, loss 0.606\n",
      "epoch 0/5, iter 507/64, loss 0.524\n",
      "epoch 0/5, iter 508/64, loss 0.296\n",
      "epoch 0/5, iter 509/64, loss 0.397\n",
      "epoch 0/5, iter 510/64, loss 0.312\n",
      "epoch 0/5, iter 511/64, loss 0.387\n",
      "epoch 0/5, iter 512/64, loss 0.549\n",
      "epoch 0/5, iter 513/64, loss 0.411\n",
      "epoch 0/5, iter 514/64, loss 0.239\n",
      "epoch 0/5, iter 515/64, loss 0.626\n",
      "epoch 0/5, iter 516/64, loss 0.330\n",
      "epoch 0/5, iter 517/64, loss 0.865\n",
      "epoch 0/5, iter 518/64, loss 0.416\n",
      "epoch 0/5, iter 519/64, loss 0.351\n",
      "epoch 0/5, iter 520/64, loss 0.585\n",
      "epoch 0/5, iter 521/64, loss 0.694\n",
      "epoch 0/5, iter 522/64, loss 0.660\n",
      "epoch 0/5, iter 523/64, loss 0.535\n",
      "epoch 0/5, iter 524/64, loss 0.462\n",
      "epoch 0/5, iter 525/64, loss 0.324\n",
      "epoch 0/5, iter 526/64, loss 0.532\n",
      "epoch 0/5, iter 527/64, loss 0.782\n",
      "epoch 0/5, iter 528/64, loss 0.439\n",
      "epoch 0/5, iter 529/64, loss 0.520\n",
      "epoch 0/5, iter 530/64, loss 0.496\n",
      "epoch 0/5, iter 531/64, loss 0.542\n",
      "epoch 0/5, iter 532/64, loss 0.346\n",
      "epoch 0/5, iter 533/64, loss 0.501\n",
      "epoch 0/5, iter 534/64, loss 0.430\n",
      "epoch 0/5, iter 535/64, loss 0.509\n",
      "epoch 0/5, iter 536/64, loss 0.605\n",
      "epoch 0/5, iter 537/64, loss 0.556\n",
      "epoch 0/5, iter 538/64, loss 0.507\n",
      "epoch 0/5, iter 539/64, loss 0.320\n",
      "epoch 0/5, iter 540/64, loss 0.554\n",
      "epoch 0/5, iter 541/64, loss 0.353\n",
      "epoch 0/5, iter 542/64, loss 0.506\n",
      "epoch 0/5, iter 543/64, loss 0.415\n",
      "epoch 0/5, iter 544/64, loss 0.422\n",
      "epoch 0/5, iter 545/64, loss 0.285\n",
      "epoch 0/5, iter 546/64, loss 0.157\n",
      "epoch 0/5, iter 547/64, loss 0.547\n",
      "epoch 0/5, iter 548/64, loss 0.407\n",
      "epoch 0/5, iter 549/64, loss 0.471\n",
      "epoch 0/5, iter 550/64, loss 0.303\n",
      "epoch 0/5, iter 551/64, loss 0.356\n",
      "epoch 0/5, iter 552/64, loss 0.487\n",
      "epoch 0/5, iter 553/64, loss 0.420\n",
      "epoch 0/5, iter 554/64, loss 0.546\n",
      "epoch 0/5, iter 555/64, loss 0.639\n",
      "epoch 0/5, iter 556/64, loss 0.431\n",
      "epoch 0/5, iter 557/64, loss 0.476\n",
      "epoch 0/5, iter 558/64, loss 0.402\n",
      "epoch 0/5, iter 559/64, loss 0.423\n",
      "epoch 0/5, iter 560/64, loss 0.429\n",
      "epoch 0/5, iter 561/64, loss 0.525\n",
      "epoch 0/5, iter 562/64, loss 0.609\n",
      "epoch 0/5, iter 563/64, loss 0.512\n",
      "epoch 0/5, iter 564/64, loss 0.572\n",
      "epoch 0/5, iter 565/64, loss 0.485\n",
      "epoch 0/5, iter 566/64, loss 0.495\n",
      "epoch 0/5, iter 567/64, loss 0.346\n",
      "epoch 0/5, iter 568/64, loss 0.221\n",
      "epoch 0/5, iter 569/64, loss 0.273\n",
      "epoch 0/5, iter 570/64, loss 0.503\n",
      "epoch 0/5, iter 571/64, loss 0.383\n",
      "epoch 0/5, iter 572/64, loss 0.607\n",
      "epoch 0/5, iter 573/64, loss 0.361\n",
      "epoch 0/5, iter 574/64, loss 0.257\n",
      "epoch 0/5, iter 575/64, loss 0.925\n",
      "epoch 0/5, iter 576/64, loss 0.320\n",
      "epoch 0/5, iter 577/64, loss 0.427\n",
      "epoch 0/5, iter 578/64, loss 0.264\n",
      "epoch 0/5, iter 579/64, loss 0.371\n",
      "epoch 0/5, iter 580/64, loss 0.391\n",
      "epoch 0/5, iter 581/64, loss 0.410\n",
      "epoch 0/5, iter 582/64, loss 0.271\n",
      "epoch 0/5, iter 583/64, loss 0.313\n",
      "epoch 0/5, iter 584/64, loss 0.451\n",
      "epoch 0/5, iter 585/64, loss 0.318\n",
      "epoch 0/5, iter 586/64, loss 0.472\n",
      "epoch 0/5, iter 587/64, loss 0.683\n",
      "epoch 0/5, iter 588/64, loss 0.270\n",
      "epoch 0/5, iter 589/64, loss 0.664\n",
      "epoch 0/5, iter 590/64, loss 0.270\n",
      "epoch 0/5, iter 591/64, loss 0.475\n",
      "epoch 0/5, iter 592/64, loss 0.315\n",
      "epoch 0/5, iter 593/64, loss 0.453\n",
      "epoch 0/5, iter 594/64, loss 0.520\n",
      "epoch 0/5, iter 595/64, loss 0.490\n",
      "epoch 0/5, iter 596/64, loss 0.331\n",
      "epoch 0/5, iter 597/64, loss 0.379\n",
      "epoch 0/5, iter 598/64, loss 0.495\n",
      "epoch 0/5, iter 599/64, loss 0.302\n",
      "epoch 0/5, iter 600/64, loss 0.531\n",
      "epoch 0/5, iter 601/64, loss 0.281\n",
      "epoch 0/5, iter 602/64, loss 0.301\n",
      "epoch 0/5, iter 603/64, loss 0.393\n",
      "epoch 0/5, iter 604/64, loss 0.498\n",
      "epoch 0/5, iter 605/64, loss 0.521\n",
      "epoch 0/5, iter 606/64, loss 0.411\n",
      "epoch 0/5, iter 607/64, loss 0.328\n",
      "epoch 0/5, iter 608/64, loss 0.529\n",
      "epoch 0/5, iter 609/64, loss 0.497\n",
      "epoch 0/5, iter 610/64, loss 0.538\n",
      "epoch 0/5, iter 611/64, loss 0.444\n",
      "epoch 0/5, iter 612/64, loss 0.488\n",
      "epoch 0/5, iter 613/64, loss 0.417\n",
      "epoch 0/5, iter 614/64, loss 0.369\n",
      "epoch 0/5, iter 615/64, loss 0.451\n",
      "epoch 0/5, iter 616/64, loss 0.279\n",
      "epoch 0/5, iter 617/64, loss 0.485\n",
      "epoch 0/5, iter 618/64, loss 0.387\n",
      "epoch 0/5, iter 619/64, loss 0.328\n",
      "epoch 0/5, iter 620/64, loss 0.308\n",
      "epoch 0/5, iter 621/64, loss 0.311\n",
      "epoch 0/5, iter 622/64, loss 0.380\n",
      "epoch 0/5, iter 623/64, loss 0.275\n",
      "epoch 0/5, iter 624/64, loss 0.381\n",
      "epoch 0/5, iter 625/64, loss 0.310\n",
      "epoch 0/5, iter 626/64, loss 0.391\n",
      "epoch 0/5, iter 627/64, loss 0.367\n",
      "epoch 0/5, iter 628/64, loss 0.223\n",
      "epoch 0/5, iter 629/64, loss 0.243\n",
      "epoch 0/5, iter 630/64, loss 0.428\n",
      "epoch 0/5, iter 631/64, loss 0.218\n",
      "epoch 0/5, iter 632/64, loss 0.228\n",
      "epoch 0/5, iter 633/64, loss 0.543\n",
      "epoch 0/5, iter 634/64, loss 0.561\n",
      "epoch 0/5, iter 635/64, loss 0.398\n",
      "epoch 0/5, iter 636/64, loss 0.313\n",
      "epoch 0/5, iter 637/64, loss 0.320\n",
      "epoch 0/5, iter 638/64, loss 0.327\n",
      "epoch 0/5, iter 639/64, loss 0.548\n",
      "epoch 0/5, iter 640/64, loss 0.503\n",
      "epoch 0/5, iter 641/64, loss 0.614\n",
      "epoch 0/5, iter 642/64, loss 0.453\n",
      "epoch 0/5, iter 643/64, loss 0.520\n",
      "epoch 0/5, iter 644/64, loss 0.481\n",
      "epoch 0/5, iter 645/64, loss 0.290\n",
      "epoch 0/5, iter 646/64, loss 0.291\n",
      "epoch 0/5, iter 647/64, loss 0.391\n",
      "epoch 0/5, iter 648/64, loss 0.411\n",
      "epoch 0/5, iter 649/64, loss 0.368\n",
      "epoch 0/5, iter 650/64, loss 0.380\n",
      "epoch 0/5, iter 651/64, loss 0.457\n",
      "epoch 0/5, iter 652/64, loss 0.534\n",
      "epoch 0/5, iter 653/64, loss 0.415\n",
      "epoch 0/5, iter 654/64, loss 0.591\n",
      "epoch 0/5, iter 655/64, loss 0.494\n",
      "epoch 0/5, iter 656/64, loss 0.509\n",
      "epoch 0/5, iter 657/64, loss 0.341\n",
      "epoch 0/5, iter 658/64, loss 0.328\n",
      "epoch 0/5, iter 659/64, loss 0.486\n",
      "epoch 0/5, iter 660/64, loss 0.359\n",
      "epoch 0/5, iter 661/64, loss 0.300\n",
      "epoch 0/5, iter 662/64, loss 0.292\n",
      "epoch 0/5, iter 663/64, loss 0.454\n",
      "epoch 0/5, iter 664/64, loss 0.269\n",
      "epoch 0/5, iter 665/64, loss 0.498\n",
      "epoch 0/5, iter 666/64, loss 0.307\n",
      "epoch 0/5, iter 667/64, loss 0.315\n",
      "epoch 0/5, iter 668/64, loss 0.444\n",
      "epoch 0/5, iter 669/64, loss 0.375\n",
      "epoch 0/5, iter 670/64, loss 0.237\n",
      "epoch 0/5, iter 671/64, loss 0.468\n",
      "epoch 0/5, iter 672/64, loss 0.198\n",
      "epoch 0/5, iter 673/64, loss 0.328\n",
      "epoch 0/5, iter 674/64, loss 0.574\n",
      "epoch 0/5, iter 675/64, loss 0.393\n",
      "epoch 0/5, iter 676/64, loss 0.516\n",
      "epoch 0/5, iter 677/64, loss 0.521\n",
      "epoch 0/5, iter 678/64, loss 0.375\n",
      "epoch 0/5, iter 679/64, loss 0.453\n",
      "epoch 0/5, iter 680/64, loss 0.339\n",
      "epoch 0/5, iter 681/64, loss 0.372\n",
      "epoch 0/5, iter 682/64, loss 0.358\n",
      "epoch 0/5, iter 683/64, loss 0.357\n",
      "epoch 0/5, iter 684/64, loss 0.508\n",
      "epoch 0/5, iter 685/64, loss 0.482\n",
      "epoch 0/5, iter 686/64, loss 0.574\n",
      "epoch 0/5, iter 687/64, loss 0.377\n",
      "epoch 0/5, iter 688/64, loss 0.300\n",
      "epoch 0/5, iter 689/64, loss 0.601\n",
      "epoch 0/5, iter 690/64, loss 0.387\n",
      "epoch 0/5, iter 691/64, loss 0.611\n",
      "epoch 0/5, iter 692/64, loss 0.270\n",
      "epoch 0/5, iter 693/64, loss 0.480\n",
      "epoch 0/5, iter 694/64, loss 0.401\n",
      "epoch 0/5, iter 695/64, loss 0.344\n",
      "epoch 0/5, iter 696/64, loss 0.421\n",
      "epoch 0/5, iter 697/64, loss 0.323\n",
      "epoch 0/5, iter 698/64, loss 0.565\n",
      "epoch 0/5, iter 699/64, loss 0.581\n",
      "epoch 0/5, iter 700/64, loss 0.401\n",
      "epoch 0/5, iter 701/64, loss 0.466\n",
      "epoch 0/5, iter 702/64, loss 0.316\n",
      "epoch 0/5, iter 703/64, loss 0.403\n",
      "epoch 0/5, iter 704/64, loss 0.309\n",
      "epoch 0/5, iter 705/64, loss 0.261\n",
      "epoch 0/5, iter 706/64, loss 0.520\n",
      "epoch 0/5, iter 707/64, loss 0.328\n",
      "epoch 0/5, iter 708/64, loss 0.438\n",
      "epoch 0/5, iter 709/64, loss 0.401\n",
      "epoch 0/5, iter 710/64, loss 0.388\n",
      "epoch 0/5, iter 711/64, loss 0.491\n",
      "epoch 0/5, iter 712/64, loss 0.348\n",
      "epoch 0/5, iter 713/64, loss 0.353\n",
      "epoch 0/5, iter 714/64, loss 0.559\n",
      "epoch 0/5, iter 715/64, loss 0.710\n",
      "epoch 0/5, iter 716/64, loss 0.209\n",
      "epoch 0/5, iter 717/64, loss 0.268\n",
      "epoch 0/5, iter 718/64, loss 0.468\n",
      "epoch 0/5, iter 719/64, loss 0.423\n",
      "epoch 0/5, iter 720/64, loss 0.385\n",
      "epoch 0/5, iter 721/64, loss 0.240\n",
      "epoch 0/5, iter 722/64, loss 0.290\n",
      "epoch 0/5, iter 723/64, loss 0.400\n",
      "epoch 0/5, iter 724/64, loss 0.299\n",
      "epoch 0/5, iter 725/64, loss 0.555\n",
      "epoch 0/5, iter 726/64, loss 0.219\n",
      "epoch 0/5, iter 727/64, loss 0.350\n",
      "epoch 0/5, iter 728/64, loss 0.346\n",
      "epoch 0/5, iter 729/64, loss 0.378\n",
      "epoch 0/5, iter 730/64, loss 0.277\n",
      "epoch 0/5, iter 731/64, loss 0.329\n",
      "epoch 0/5, iter 732/64, loss 0.352\n",
      "epoch 0/5, iter 733/64, loss 0.417\n",
      "epoch 0/5, iter 734/64, loss 0.637\n",
      "epoch 0/5, iter 735/64, loss 0.361\n",
      "epoch 0/5, iter 736/64, loss 0.438\n",
      "epoch 0/5, iter 737/64, loss 0.462\n",
      "epoch 0/5, iter 738/64, loss 0.377\n",
      "epoch 0/5, iter 739/64, loss 0.410\n",
      "epoch 0/5, iter 740/64, loss 0.387\n",
      "epoch 0/5, iter 741/64, loss 0.344\n",
      "epoch 0/5, iter 742/64, loss 0.491\n",
      "epoch 0/5, iter 743/64, loss 0.252\n",
      "epoch 0/5, iter 744/64, loss 0.467\n",
      "epoch 0/5, iter 745/64, loss 0.394\n",
      "epoch 0/5, iter 746/64, loss 0.423\n",
      "epoch 0/5, iter 747/64, loss 0.402\n",
      "epoch 0/5, iter 748/64, loss 0.436\n",
      "epoch 0/5, iter 749/64, loss 0.210\n",
      "epoch 0/5, iter 750/64, loss 0.356\n",
      "epoch 0/5, iter 751/64, loss 0.257\n",
      "epoch 0/5, iter 752/64, loss 0.632\n",
      "epoch 0/5, iter 753/64, loss 0.321\n",
      "epoch 0/5, iter 754/64, loss 0.304\n",
      "epoch 0/5, iter 755/64, loss 0.266\n",
      "epoch 0/5, iter 756/64, loss 0.252\n",
      "epoch 0/5, iter 757/64, loss 0.422\n",
      "epoch 0/5, iter 758/64, loss 0.302\n",
      "epoch 0/5, iter 759/64, loss 0.316\n",
      "epoch 0/5, iter 760/64, loss 0.250\n",
      "epoch 0/5, iter 761/64, loss 0.275\n",
      "epoch 0/5, iter 762/64, loss 0.366\n",
      "epoch 0/5, iter 763/64, loss 0.452\n",
      "epoch 0/5, iter 764/64, loss 0.611\n",
      "epoch 0/5, iter 765/64, loss 0.320\n",
      "epoch 0/5, iter 766/64, loss 0.266\n",
      "epoch 0/5, iter 767/64, loss 0.335\n",
      "epoch 0/5, iter 768/64, loss 0.309\n",
      "epoch 0/5, iter 769/64, loss 0.272\n",
      "epoch 0/5, iter 770/64, loss 0.197\n",
      "epoch 0/5, iter 771/64, loss 0.489\n",
      "epoch 0/5, iter 772/64, loss 0.646\n",
      "epoch 0/5, iter 773/64, loss 0.457\n",
      "epoch 0/5, iter 774/64, loss 0.258\n",
      "epoch 0/5, iter 775/64, loss 0.353\n",
      "epoch 0/5, iter 776/64, loss 0.293\n",
      "epoch 0/5, iter 777/64, loss 0.239\n",
      "epoch 0/5, iter 778/64, loss 0.494\n",
      "epoch 0/5, iter 779/64, loss 0.313\n",
      "epoch 0/5, iter 780/64, loss 0.375\n",
      "epoch 0/5, iter 781/64, loss 0.389\n",
      "epoch 0/5, iter 782/64, loss 0.308\n",
      "epoch 0/5, iter 783/64, loss 0.410\n",
      "epoch 0/5, iter 784/64, loss 0.223\n",
      "epoch 0/5, iter 785/64, loss 0.259\n",
      "epoch 0/5, iter 786/64, loss 0.378\n",
      "epoch 0/5, iter 787/64, loss 0.331\n",
      "epoch 0/5, iter 788/64, loss 0.192\n",
      "epoch 0/5, iter 789/64, loss 0.153\n",
      "epoch 0/5, iter 790/64, loss 0.364\n",
      "epoch 0/5, iter 791/64, loss 0.406\n",
      "epoch 0/5, iter 792/64, loss 0.480\n",
      "epoch 0/5, iter 793/64, loss 0.334\n",
      "epoch 0/5, iter 794/64, loss 0.320\n",
      "epoch 0/5, iter 795/64, loss 0.264\n",
      "epoch 0/5, iter 796/64, loss 0.552\n",
      "epoch 0/5, iter 797/64, loss 0.577\n",
      "epoch 0/5, iter 798/64, loss 0.425\n",
      "epoch 0/5, iter 799/64, loss 0.510\n",
      "epoch 0/5, iter 800/64, loss 0.402\n",
      "epoch 0/5, iter 801/64, loss 0.482\n",
      "epoch 0/5, iter 802/64, loss 0.520\n",
      "epoch 0/5, iter 803/64, loss 0.512\n",
      "epoch 0/5, iter 804/64, loss 0.323\n",
      "epoch 0/5, iter 805/64, loss 0.345\n",
      "epoch 0/5, iter 806/64, loss 0.462\n",
      "epoch 0/5, iter 807/64, loss 0.508\n",
      "epoch 0/5, iter 808/64, loss 0.449\n",
      "epoch 0/5, iter 809/64, loss 0.409\n",
      "epoch 0/5, iter 810/64, loss 0.219\n",
      "epoch 0/5, iter 811/64, loss 0.339\n",
      "epoch 0/5, iter 812/64, loss 0.232\n",
      "epoch 0/5, iter 813/64, loss 0.277\n",
      "epoch 0/5, iter 814/64, loss 0.472\n",
      "epoch 0/5, iter 815/64, loss 0.495\n",
      "epoch 0/5, iter 816/64, loss 0.456\n",
      "epoch 0/5, iter 817/64, loss 0.398\n",
      "epoch 0/5, iter 818/64, loss 0.370\n",
      "epoch 0/5, iter 819/64, loss 0.433\n",
      "epoch 0/5, iter 820/64, loss 0.356\n",
      "epoch 0/5, iter 821/64, loss 0.707\n",
      "epoch 0/5, iter 822/64, loss 0.350\n",
      "epoch 0/5, iter 823/64, loss 0.304\n",
      "epoch 0/5, iter 824/64, loss 0.374\n",
      "epoch 0/5, iter 825/64, loss 0.363\n",
      "epoch 0/5, iter 826/64, loss 0.380\n",
      "epoch 0/5, iter 827/64, loss 0.235\n",
      "epoch 0/5, iter 828/64, loss 0.450\n",
      "epoch 0/5, iter 829/64, loss 0.378\n",
      "epoch 0/5, iter 830/64, loss 0.351\n",
      "epoch 0/5, iter 831/64, loss 0.407\n",
      "epoch 0/5, iter 832/64, loss 0.487\n",
      "epoch 0/5, iter 833/64, loss 0.312\n",
      "epoch 0/5, iter 834/64, loss 0.442\n",
      "epoch 0/5, iter 835/64, loss 0.317\n",
      "epoch 0/5, iter 836/64, loss 0.498\n",
      "epoch 0/5, iter 837/64, loss 0.225\n",
      "epoch 0/5, iter 838/64, loss 0.245\n",
      "epoch 0/5, iter 839/64, loss 0.201\n",
      "epoch 0/5, iter 840/64, loss 0.319\n",
      "epoch 0/5, iter 841/64, loss 0.510\n",
      "epoch 0/5, iter 842/64, loss 0.329\n",
      "epoch 0/5, iter 843/64, loss 0.362\n",
      "epoch 0/5, iter 844/64, loss 0.341\n",
      "epoch 0/5, iter 845/64, loss 0.490\n",
      "epoch 0/5, iter 846/64, loss 0.282\n",
      "epoch 0/5, iter 847/64, loss 0.283\n",
      "epoch 0/5, iter 848/64, loss 0.464\n",
      "epoch 0/5, iter 849/64, loss 0.350\n",
      "epoch 0/5, iter 850/64, loss 0.590\n",
      "epoch 0/5, iter 851/64, loss 0.431\n",
      "epoch 0/5, iter 852/64, loss 0.469\n",
      "epoch 0/5, iter 853/64, loss 0.248\n",
      "epoch 0/5, iter 854/64, loss 0.351\n",
      "epoch 0/5, iter 855/64, loss 0.489\n",
      "epoch 0/5, iter 856/64, loss 0.180\n",
      "epoch 0/5, iter 857/64, loss 0.255\n",
      "epoch 0/5, iter 858/64, loss 0.352\n",
      "epoch 0/5, iter 859/64, loss 0.373\n",
      "epoch 0/5, iter 860/64, loss 0.256\n",
      "epoch 0/5, iter 861/64, loss 0.384\n",
      "epoch 0/5, iter 862/64, loss 0.150\n",
      "epoch 0/5, iter 863/64, loss 0.315\n",
      "epoch 0/5, iter 864/64, loss 0.324\n",
      "epoch 0/5, iter 865/64, loss 0.228\n",
      "epoch 0/5, iter 866/64, loss 0.416\n",
      "epoch 0/5, iter 867/64, loss 0.312\n",
      "epoch 0/5, iter 868/64, loss 0.388\n",
      "epoch 0/5, iter 869/64, loss 0.181\n",
      "epoch 0/5, iter 870/64, loss 0.277\n",
      "epoch 0/5, iter 871/64, loss 0.320\n",
      "epoch 0/5, iter 872/64, loss 0.264\n",
      "epoch 0/5, iter 873/64, loss 0.440\n",
      "epoch 0/5, iter 874/64, loss 0.305\n",
      "epoch 0/5, iter 875/64, loss 0.498\n",
      "epoch 0/5, iter 876/64, loss 0.328\n",
      "epoch 0/5, iter 877/64, loss 0.501\n",
      "epoch 0/5, iter 878/64, loss 0.459\n",
      "epoch 0/5, iter 879/64, loss 0.387\n",
      "epoch 0/5, iter 880/64, loss 0.372\n",
      "epoch 0/5, iter 881/64, loss 0.357\n",
      "epoch 0/5, iter 882/64, loss 0.288\n",
      "epoch 0/5, iter 883/64, loss 0.416\n",
      "epoch 0/5, iter 884/64, loss 0.187\n",
      "epoch 0/5, iter 885/64, loss 0.258\n",
      "epoch 0/5, iter 886/64, loss 0.294\n",
      "epoch 0/5, iter 887/64, loss 0.488\n",
      "epoch 0/5, iter 888/64, loss 0.525\n",
      "epoch 0/5, iter 889/64, loss 0.380\n",
      "epoch 0/5, iter 890/64, loss 0.329\n",
      "epoch 0/5, iter 891/64, loss 0.211\n",
      "epoch 0/5, iter 892/64, loss 0.220\n",
      "epoch 0/5, iter 893/64, loss 0.301\n",
      "epoch 0/5, iter 894/64, loss 0.183\n",
      "epoch 0/5, iter 895/64, loss 0.285\n",
      "epoch 0/5, iter 896/64, loss 0.403\n",
      "epoch 0/5, iter 897/64, loss 0.379\n",
      "epoch 0/5, iter 898/64, loss 0.376\n",
      "epoch 0/5, iter 899/64, loss 0.192\n",
      "epoch 0/5, iter 900/64, loss 0.235\n",
      "epoch 0/5, iter 901/64, loss 0.296\n",
      "epoch 0/5, iter 902/64, loss 0.283\n",
      "epoch 0/5, iter 903/64, loss 0.212\n",
      "epoch 0/5, iter 904/64, loss 0.475\n",
      "epoch 0/5, iter 905/64, loss 0.359\n",
      "epoch 0/5, iter 906/64, loss 0.303\n",
      "epoch 0/5, iter 907/64, loss 0.392\n",
      "epoch 0/5, iter 908/64, loss 0.316\n",
      "epoch 0/5, iter 909/64, loss 0.295\n",
      "epoch 0/5, iter 910/64, loss 0.360\n",
      "epoch 0/5, iter 911/64, loss 0.652\n",
      "epoch 0/5, iter 912/64, loss 0.418\n",
      "epoch 0/5, iter 913/64, loss 0.292\n",
      "epoch 0/5, iter 914/64, loss 0.298\n",
      "epoch 0/5, iter 915/64, loss 0.357\n",
      "epoch 0/5, iter 916/64, loss 0.401\n",
      "epoch 0/5, iter 917/64, loss 0.271\n",
      "epoch 0/5, iter 918/64, loss 0.363\n",
      "epoch 0/5, iter 919/64, loss 0.453\n",
      "epoch 0/5, iter 920/64, loss 0.282\n",
      "epoch 0/5, iter 921/64, loss 0.439\n",
      "epoch 0/5, iter 922/64, loss 0.258\n",
      "epoch 0/5, iter 923/64, loss 0.287\n",
      "epoch 0/5, iter 924/64, loss 0.416\n",
      "epoch 0/5, iter 925/64, loss 0.325\n",
      "epoch 0/5, iter 926/64, loss 0.398\n",
      "epoch 0/5, iter 927/64, loss 0.530\n",
      "epoch 0/5, iter 928/64, loss 0.406\n",
      "epoch 0/5, iter 929/64, loss 0.240\n",
      "epoch 0/5, iter 930/64, loss 0.383\n",
      "epoch 0/5, iter 931/64, loss 0.478\n",
      "epoch 0/5, iter 932/64, loss 0.288\n",
      "epoch 0/5, iter 933/64, loss 0.344\n",
      "epoch 0/5, iter 934/64, loss 0.254\n",
      "epoch 0/5, iter 935/64, loss 0.462\n",
      "epoch 0/5, iter 936/64, loss 0.379\n",
      "epoch 0/5, iter 937/64, loss 0.357\n",
      "epoch 1, loss 0.5910, train acc 0.779, test acc 0.871, time 1515.2 sec\n",
      "epoch 1/5, iter 0/64, loss 0.323\n",
      "epoch 1/5, iter 1/64, loss 0.250\n",
      "epoch 1/5, iter 2/64, loss 0.331\n",
      "epoch 1/5, iter 3/64, loss 0.362\n"
     ]
    }
   ]
  },
  {
   "source": [
    "VGG-11通过5个可以重复使用的卷积块来构造网络。\n",
    "\n",
    "根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}