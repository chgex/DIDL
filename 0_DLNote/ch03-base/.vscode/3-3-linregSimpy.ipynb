{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 线性回归的简洁实现\n",
    "\n",
    "利用pytorch工具包来实现线性模型：\n",
    "\n",
    "1. `torch.utils.data`模块提供有关数据处理的工具\n",
    "\n",
    "2. `torch.nn`模块定义了大量神经网络的层\n",
    "\n",
    "3. `troch.nn.init`提供了常用的优化算法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 生成数据集\n",
    "\n",
    "num_example=1000\n",
    "num_feature=2\n",
    "\n",
    "true_w=[2,3.4]\n",
    "true_b=4.2\n",
    "\n",
    "# dataArr\n",
    "dataArr=torch.tensor(np.random.normal(0,1,\n",
    "    size=(num_example,num_feature,)),\n",
    "    dtype=torch.float)\n",
    "# labels\n",
    "labels=true_w[0]*dataArr[:,0]+true_w[1]*dataArr[:,1]+true_b\n",
    "\n",
    "# noise\n",
    "labels+=torch.tensor(np.random.normal(0,0.01,size=labels.size()),dtype=torch.float)"
   ]
  },
  {
   "source": [
    "**读取数据**\n",
    "\n",
    "使用`torch.utils.data`包，读取feature和labels为数据和标签的组合。\n",
    "\n",
    "使用`data.DataLoader(dataset,batch_size,)`获取batch_size的数据。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "# 一次读取batch_size个数据\n",
    "batch_size=10\n",
    "\n",
    "# 组合数据特征和标签\n",
    "dataset=Data.TensorDataset(dataArr,labels)\n",
    "\n",
    "# 随机读取批量数据\n",
    "data_iter=Data.DataLoader(dataset,batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.5116,  0.4040],\n        [ 0.6109,  1.3295],\n        [-1.5260,  1.2462],\n        [ 1.3314,  0.1926],\n        [-0.2202, -0.0371],\n        [ 0.7827,  0.8682],\n        [-0.0138, -0.9928],\n        [ 0.5717, -0.4736],\n        [-0.2960, -2.2792],\n        [ 0.3316, -0.1090]]) tensor([ 4.5505,  9.9601,  5.3854,  7.5171,  3.6353,  8.7261,  0.7932,  3.7302,\n        -4.1464,  4.4881])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    # 打印一次就可以了\n",
    "    break "
   ]
  },
  {
   "source": [
    "### 定义模型\n",
    "\n",
    "`torch.nn`核心数据结构是`Module`，是个抽象类。\n",
    "\n",
    "一个`nn.Module`实例应该包含以一些层以及前向传播方法，这些前向传播方法就是输出。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**方法1**\n",
    "\n",
    "使用`nn.Module`定义模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self,n_feature):\n",
    "        super(LinearNet,self).__init__()\n",
    "        self.linear=nn.Linear(n_feature,1)\n",
    "\n",
    "    # 定义前向传播\n",
    "    def forward(self,x):\n",
    "        y=self.linear(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LinearNet(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net1=LinearNet(num_feature)\n",
    "print(net1)"
   ]
  },
  {
   "source": [
    "**方法2**\n",
    "\n",
    "使用`nn.Sequential`定义模型\n",
    "\n",
    "网络层次将会按照添加顺序，自动构造生成。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (0): Linear(in_features=2, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net2=nn.Sequential( nn.Linear(num_feature,1) \n",
    "    # other layer\n",
    "    )\n",
    "print(net2)"
   ]
  },
  {
   "source": [
    "**方法2另一种写法**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n  (linear): Linear(in_features=2, out_features=1, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net3=nn.Sequential()\n",
    "\n",
    "# 添加层\n",
    "net3.add_module('linear',nn.Linear(num_feature,1))\n",
    "# net.add(...)\n",
    "print(net3)\n"
   ]
  },
  {
   "source": [
    "Note: 线性回顾输出层中的神经元和输入层中各个输入层完全连接，所以，线性回归的输出层又叫做全连接层。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "**初始化模型参数**\n",
    "\n",
    "使用`torch.nn.init`初始化模型参数。\n",
    "\n",
    "如果使用net1,那么`net[0].weight`应该写为`net[0].linear.weight`，bias同。\n",
    "\n",
    "使用`net[0].weight`访问，只有当net是`ModuleList`或`Sequential`时才可以。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "net=net2\n",
    "# 初始化权重为：均值为0，方差为0.01的正态分布\n",
    "init.normal_(net[0].weight,mean=0,std=0.01)\n",
    "init.constant(net[0].bias,val=0)"
   ]
  },
  {
   "source": [
    "###  定义损失函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 均方根误差\n",
    "loss=nn.MSELoss()"
   ]
  },
  {
   "source": [
    "### 定义优化算法\n",
    "\n",
    "使用`torch.optim`模块，该模块提供了许多优化算法：SGD,Adam,RMSProp等。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.03\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# ls为学习率\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.03)\n",
    "\n",
    "print(optimizer)"
   ]
  },
  {
   "source": [
    "**其它**\n",
    "\n",
    "不同子网设置不同学习率：\n",
    "\n",
    "```python\n",
    "optimizer =optim.SGD(\n",
    "# 如果对某个参数不指定学习率，就使用最外层的默认学习率\n",
    "    [{'params': net.subnet1.parameters()}, # lr=0.03\n",
    "    {'params': net.subnet2.parameters(), 'lr': 0.01}], \n",
    "    lr=0.03)\n",
    "print(optimizer)\n",
    "```\n",
    "\n",
    "可以动态调整学习率：\n",
    "\n",
    "```python\n",
    "for param_group in optimizer.param_group:\n",
    "    param_group['lr']*=0.1\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 训练模型\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss:0.000312\n",
      "epoch 2, loss:0.000046\n",
      "epoch 3, loss:0.000035\n"
     ]
    }
   ],
   "source": [
    "num_epoch=3\n",
    "\n",
    "for epoch in range(1,num_epoch+1):\n",
    "    for X,y in data_iter:\n",
    "        out=net(X)\n",
    "        l=loss(out,y.view(-1,1))\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 反向传播\n",
    "        l.backward()\n",
    "        # optim.step函数，用来迭代模型参数，对批量中样本梯度求平均\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss:%f' % (epoch,l.item()))\n"
   ]
  },
  {
   "source": [
    "### 参数比较\n",
    "\n",
    "从`net`获得想要的层，然后就可以直接访问权重和偏差。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2, 3.4] \n Parameter containing:\ntensor([[1.9994, 3.3999]], requires_grad=True)\n4.2 \n Parameter containing:\ntensor([4.2000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "dense=net[0]\n",
    "\n",
    "print(true_w,'\\n',dense.weight)\n",
    "print(true_b,'\\n',dense.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}