{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acd847c392487aabfa03d14b5dc5b2ae233417a28e2d9e43c03b69bccff2848e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 3.13 丢弃法\n",
    "\n",
    "该方法用于应对过拟合问题。\n",
    "\n",
    "丢弃法不改变输入的期望值。\n",
    "\n",
    "在troch.nn.Module中使用dropout，可以简洁实现。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import d2l_pytorch as d2l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型,在relue之后引入dropout\n",
    "\n",
    "# 包含两个隐藏层的MLP，每个隐藏层的输出个数都是256\n",
    "\n",
    "num_epochs, lr, batch_size = 5, 100.0, 256\n",
    "\n",
    "drop_plob1,drop_plob2=0.2,0.5\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "net=nn.Sequential(\n",
    "    d2l.FlattenLayer(),\n",
    "    nn.Linear(num_inputs,num_hiddens1),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_plob2),\n",
    "    nn.Linear(num_hiddens1,num_hiddens2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(drop_plob2),\n",
    "    nn.Linear(num_hiddens2,10)\n",
    ")\n",
    "# 初始化模型的参数\n",
    "for param in net.parameters():\n",
    "    nn.init.normal_(param,mean=0,std=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器，\n",
    "# 就是更新梯度的\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "\n",
    "train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.0044, train acc 0.568,test acc 0.692\n",
      "epoch 2, loss 0.0024, train acc 0.774,test acc 0.739\n",
      "epoch 3, loss 0.0021, train acc 0.808,test acc 0.836\n",
      "epoch 4, loss 0.0019, train acc 0.827,test acc 0.840\n",
      "epoch 5, loss 0.0018, train acc 0.837,test acc 0.807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 训练模型\n",
    "\n",
    "d2l.train_ch03(net, train_iter, test_iter, loss,batch_size, num_epochs,  None, None, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}